{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 784,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'nlp_jtk' from 'nlp_jtk.pyc'>"
      ]
     },
     "execution_count": 784,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from Parser import *\n",
    "import os\n",
    "import statistics as s\n",
    "from nlp_jtk import Token, Sentence\n",
    "import codecs\n",
    "import Parser\n",
    "reload(Parser)\n",
    "import nlp_jtk\n",
    "reload(nlp_jtk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 785,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Language(object):\n",
    "    def __init__(self, name, tagged_file_list=None):\n",
    "        self.tagger_train_iters = 3\n",
    "        self.parser_train_iters = 6 #?\n",
    "        self.name = name\n",
    "        if tagged_file_list is None:\n",
    "            tagged_file_list = []\n",
    "        self.tagged_file_list=tagged_file_list\n",
    "        \n",
    "        self.supervised_parser = Spacy_Parser()\n",
    "        self.unsupervised_parser = Spacy_Parser()\n",
    "        self.train_data_file = None\n",
    "        \n",
    "    def setup(self, train=False, test=False):\n",
    "        if train:\n",
    "            print \"Training supervised \"+self.name+\" Tagger.\"\n",
    "            self.train_supervised_tagger()\n",
    "            print \"Training supervised \"+self.name+\" Parser.\"\n",
    "            self.train_supervised_parser()\n",
    "                \n",
    "        if test:\n",
    "            print self.name + \" test results: \"\n",
    "            self.test_supervised()\n",
    "        \n",
    "    def train_supervised_tagger(self):\n",
    "        self.train_data = self.get_train_data_set()\n",
    "        #print str(len(self.train_data)) + \" tagger training sentences\"\n",
    "        #print str(self.tagger_train_iters) + \" training iterations\"\n",
    "        #self.supervised_parser.tagger.train(self.train_data, nr_iter=self.tagger_train_iters)\n",
    "        \n",
    "    def train_supervised_parser(self):\n",
    "        if not self.train_data:\n",
    "            print \"need to train tagger first\"\n",
    "            return\n",
    "        \n",
    "        print str(len(self.train_data)) + \" parser training sentences\"\n",
    "        print str(self.parser_train_iters) + \" training iterations\"\n",
    "        self.supervised_parser.train(self.train_data, nr_iter=self.parser_train_iters)\n",
    "        \n",
    "    def test_supervised(self):\n",
    "        test_correct_data = self.get_test_data_set()\n",
    "        test_guess = self.supervised_parser.parse(self.convert_sentence_list_to_untagged_corpus(test_correct_data))\n",
    "        self.get_test_results(test_guess, test_correct_data)\n",
    "        \n",
    "    \n",
    "    def test_unsupervised(self):\n",
    "        test_correct_data = self.get_test_data_set()\n",
    "        test_guess = self.unsupervised_parser.parse(self.convert_sentence_list_to_untagged_corpus(test_correct_data))\n",
    "        self.get_test_results(test_guess, test_correct_data)\n",
    "        pass\n",
    "    \n",
    "    def get_test_results(self, guess_tags, correct_tags):\n",
    "        tag_score_dict = {}\n",
    "        \n",
    "        correct_tag_type ={}\n",
    "        wrong_tag_type = {}\n",
    "    \n",
    "        conf_right = []\n",
    "        conf_wrong = []\n",
    "    \n",
    "        total_tags = 0\n",
    "        total_wrong_tags = 0\n",
    "    \n",
    "        total_sentences = len(guess_tags)\n",
    "        total_wrong_sent = 0\n",
    "    \n",
    "        for sent_num, correct_sentence in enumerate(correct_tags):\n",
    "\n",
    "            perfect_sentence = True\n",
    "            for word_idx, correct_token in enumerate(correct_sentence.get_tokens()):\n",
    "                guess_token = guess_tags[sent_num].get_token_at(word_idx)\n",
    "                assert correct_token.orig == guess_token.orig\n",
    "                \n",
    "                for i, (feature, guess) in enumerate(guess_token.get_testable_attr_list()):\n",
    "                    tag_score_dict[feature] = tag_score_dict.get(feature, 0) + (guess==correct_token.get_testable_attr_list()[i][1])\n",
    "                \n",
    "                tag_guess = guess_token.pos_tag\n",
    "                guess_confidence = guess_token.conf\n",
    "                total_tags +=1\n",
    "            \n",
    "                if(correct_token.pos_tag != tag_guess):\n",
    "                    total_wrong_tags +=1\n",
    "                    conf_wrong.append(guess_confidence)\n",
    "                    perfect_sentence = False\n",
    "                    error_tuple = (correct_token.pos_tag, tag_guess)\n",
    "                    wrong_tag_type[error_tuple] = wrong_tag_type.get(error_tuple, 0) + 1\n",
    "                else:\n",
    "                    correct_tag_type[tag_guess] = correct_tag_type.get(tag_guess, 0) + 1\n",
    "                    conf_right.append(guess_confidence)\n",
    "                \n",
    "            if not perfect_sentence:\n",
    "                total_wrong_sent+= 1\n",
    "                \n",
    "        if(len(conf_right) >0 and len(conf_wrong)>0): \n",
    "            print \"average confidence of right tag= \" + str(s.mean(conf_right))\n",
    "            print \"average confidence of wrong tag= \" + str(s.mean(conf_wrong))\n",
    "            print \"stdev confidence of right tag= \" + str(s.stdev(conf_right))\n",
    "            print \"stdev confidence of wrong tag= \" + str(s.stdev(conf_wrong))\n",
    "   \n",
    "        tag_word_acc = (100.00*(total_tags-total_wrong_tags))/total_tags\n",
    "        tag_sentence_acc = (100.00*(total_sentences-total_wrong_sent))/total_sentences\n",
    "\n",
    "        print \"tag token accuracy: \" + str(tag_word_acc) + \"%\"\n",
    "        print \"tag sentence accuracy: \" + str(tag_sentence_acc) + \"%\"\n",
    "        print \"have not written tests for parse yet\"\n",
    "        for feature, correct_count in tag_score_dict.iteritems():\n",
    "            print feature, \"accuracy:\", (100.0*correct_count)/total_tags\n",
    "    \n",
    "\n",
    "    def get_tagged_sentences(self, file_name):\n",
    "        sentences_w_tags = []\n",
    "        count = 0\n",
    "        words=[]\n",
    "        tags=[]\n",
    "        sentence_obj = Sentence()\n",
    "        sentence_obj.add_token(Token(orig_token='<start>'))\n",
    "        on_sentence = False\n",
    "        for line in codecs.open(file_name, 'r', encoding=\"utf-8\"):\n",
    "        \n",
    "            vals = line.split('\\t')\n",
    "            if (len(vals) > 1):\n",
    "                on_sentence = True\n",
    "                tok = Token()\n",
    "                tok.orig = vals[1]\n",
    "                tok.pos_tag = vals[3]\n",
    "                tok.head = int(vals[6])\n",
    "                tok.head_label = vals[7]\n",
    "                sentence_obj.add_token(tok)\n",
    "            elif (on_sentence):\n",
    "                on_sentence=False\n",
    "                sentence_obj.add_token(Token(orig_token='ROOT'))\n",
    "                sentences_w_tags.append(sentence_obj)\n",
    "                sentence_obj = Sentence()\n",
    "                sentence_obj.add_token(Token(orig_token='<start>'))\n",
    "    \n",
    "        return sentences_w_tags # [ Sentence_obj, Sentence_obj]\n",
    "    def get_train_data_set(self):\n",
    "        print \"Tagged data: \" + str(len(self.tagged_file_list)) + \" files\"\n",
    "        print \"Picking Largest\"\n",
    "        large_file = \"\"\n",
    "        maxFileSize = 0\n",
    "        for f in self.tagged_file_list:\n",
    "            if os.stat(f).st_size > maxFileSize:\n",
    "                large_file = f\n",
    "                \n",
    "                maxFileSize = os.stat(f).st_size\n",
    "                self.train_data_file = f\n",
    "        print self.train_data_file\n",
    "        return self.get_tagged_sentences(large_file)\n",
    "    \n",
    "    def get_test_data_set(self):\n",
    "        test_list = []\n",
    "        for f in self.tagged_file_list:\n",
    "            if not f == self.train_data_file:\n",
    "                test_list += self.get_tagged_sentences(f)\n",
    "        return test_list\n",
    "    \n",
    "    \n",
    "    def convert_sentence_list_to_untagged_corpus(self, sentence_list):\n",
    "        import copy\n",
    "        untagged_list = copy.deepcopy(sentence_list)\n",
    "        for sent in untagged_list:\n",
    "            sent.clear_tags()\n",
    "        return untagged_list\n",
    "        \n",
    "        \n",
    "#use intern function for performance enhancement & pad tokens in the appropriate place\n",
    "def read_conll(loc):\n",
    "    for sent_str in open(loc).read().strip().split('\\n\\n'):\n",
    "        lines = [line.split() for line in sent_str.split('\\n')]\n",
    "        words = DefaultList(''); tags = DefaultList('')\n",
    "        heads = [None]; labels = [None]\n",
    "        for i, (word, pos, head, label) in enumerate(lines):\n",
    "            words.append(intern(word))\n",
    "            #words.append(intern(normalize(word)))\n",
    "            tags.append(intern(pos))\n",
    "            heads.append(int(head) + 1 if head != '-1' else len(lines) + 1)\n",
    "            labels.append(label)\n",
    "        pad_tokens(words); pad_tokens(tags)\n",
    "        yield words, tags, heads, labels\n",
    "\n",
    "\n",
    "def pad_tokens(tokens):\n",
    "    tokens.insert(0, '<start>')\n",
    "    tokens.append('ROOT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 786,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Translation(object):\n",
    "    def __init__(self, src_language, tgt_language, src_file, tgt_file, align_file):\n",
    "        self.src_language = src_language\n",
    "        self.src_file = src_file\n",
    "        \n",
    "        self.tgt_language = tgt_language\n",
    "        self.tgt_file = tgt_file\n",
    "        \n",
    "        self.align_file = align_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 787,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "en_train_file='../../Data/UD_English/en-ud-train.conllu'\n",
    "en_test_file='../../Data/UD_English/en-ud-test.conllu'\n",
    "en_3 = '../../Data/UD_English/en-ud-dev.conllu'\n",
    "en = Language(\"English\", [en_train_file, en_test_file, en_3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 788,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#en.setup(train=True, test=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 789,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tagged data: 3 files\n",
      "Picking Largest\n",
      "../../Data/UD_English/en-ud-train.conllu\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "en.train_supervised_tagger()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 790,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12543 parser training sentences\n",
      "6 training iterations\n",
      "0 0.334\n",
      "1 0.383\n",
      "2 0.403\n",
      "3 0.419\n",
      "4 0.429\n",
      "5 0.444\n",
      "Averaging weights\n"
     ]
    }
   ],
   "source": [
    "en.train_supervised_parser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 791,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average confidence of right tag= 21.2162391151\n",
      "average confidence of wrong tag= 6.49013540068\n",
      "stdev confidence of right tag= 7.08065432371\n",
      "stdev confidence of wrong tag= 6.13636207827\n",
      "tag token accuracy: 94.423136194%\n",
      "tag sentence accuracy: 56.9992645256%\n",
      "have not written tests for parse yet\n",
      "Dependency Label accuracy: 100.0\n",
      "Head Index accuracy: 46.6388137393\n",
      "POS tag accuracy: 94.423136194\n"
     ]
    }
   ],
   "source": [
    "en.test_supervised()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 792,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time:  993.265013218\n"
     ]
    }
   ],
   "source": [
    "end = time.time()\n",
    "print \"time: \", end-start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
