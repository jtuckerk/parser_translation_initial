{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'POS_Tagger' from 'POS_Tagger.pyc'>"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from POS_Tagger import PerceptronTagger, AveragedPerceptron\n",
    "reload(POS_Tagger)\n",
    "#from POS_Tagger import PerceptronTagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "class Parse(object):\n",
    "    def __init__(self, n):\n",
    "        self.n = n\n",
    "        self.heads = [None] * (n-1)\n",
    "        self.lefts = []\n",
    "        self.rights = []\n",
    "        for i in range(n+1):\n",
    "            self.lefts.append([])\n",
    "            self.rights.append([])\n",
    "    \n",
    "    def add_arc(self, head, child):\n",
    "        self.heads[child] = head\n",
    "        if child < head:\n",
    "            self.lefts[head].append(child)\n",
    "        else:\n",
    "            self.rights[head].append(child)\n",
    "            \n",
    "    SHIFT = 0; RIGHT = 1; LEFT = 2\n",
    "    MOVES = [SHIFT, RIGHT, LEFT]\n",
    " \n",
    "    def transition(move, i, stack, parse):\n",
    "        global SHIFT, RIGHT, LEFT\n",
    "        if move == SHIFT:\n",
    "            stack.append(i)\n",
    "            return i + 1\n",
    "        elif move == RIGHT:\n",
    "            parse.add_arc(stack[-2], stack.pop())\n",
    "            return i\n",
    "        elif move == LEFT:\n",
    "            parse.add_arc(i, stack.pop())\n",
    "            return i\n",
    "        raise GrammarError(\"Unknown move: %d\" % move)\n",
    "        \n",
    "class Spacy_Parser(object):\n",
    "\n",
    "    START = ['-START-', '-START2-']\n",
    "    END = ['-END-', '-END2-']\n",
    "    #AP_MODEL_LOC = os.path.join(os.path.dirname(__file__), PICKLE)\n",
    "\n",
    "    def __init__(self, load=False):\n",
    "        self.tagger = PerceptronTagger()\n",
    "        self.model = AveragedPerceptron()\n",
    "        self.tagdict = {}\n",
    "        self.classes = set()\n",
    "        \n",
    "    def train_tagger(self, sentences_with_tags, num_iters=5):\n",
    "        self.tagger.train(sentences_with_tags, nr_iter=num_iters)\n",
    "        \n",
    "    def parse(self, words):\n",
    "        sentence = self.tagger.tag(words)\n",
    "        tags = sentence.pos_tags()\n",
    "        n = len(words)\n",
    "        idx = 1\n",
    "        stack = [0]\n",
    "        deps = Parse(n)\n",
    "        while stack or idx < n:\n",
    "            features = extract_features(words, tags, idx, n, stack, deps)\n",
    "            scores = self.model.score(features)\n",
    "            valid_moves = get_valid_moves(i, n, len(stack))\n",
    "            next_move = max(valid_moves, key=lambda move: scores[move])\n",
    "            idx = transition(next_move, idx, stack, parse)\n",
    "        sentence.set_heads(parse)#still not sure what parse is\n",
    "        return sentence\n",
    " \n",
    "    def train_one(self, itn, words, gold_tags, gold_heads):\n",
    "        #spacy blog says using gold tags is not the move\n",
    "        n = len(words)\n",
    "        i = 2; stack = [1]; parse = Parse(n)\n",
    "        tags = self.tagger.tag(\" \".join(words)).pos_tags()\n",
    "        while stack or (i + 1) < n:\n",
    "            features = extract_features(words, tags, i, n, stack, parse)\n",
    "            scores = self.model.score(features)\n",
    "            valid_moves = get_valid_moves(i, n, len(stack))\n",
    "            guess = max(valid_moves, key=lambda move: scores[move])\n",
    "            gold_moves = get_gold_moves(i, n, stack, parse.heads, gold_heads)\n",
    "            best = max(gold_moves, key=lambda move: scores[move])\n",
    "        self.model.update(best, guess, features)\n",
    "        i = transition(guess, i, stack, parse)\n",
    "        # Return number correct\n",
    "        return len([i for i in range(n-1) if parse.heads[i] == gold_heads[i]])\n",
    "    \n",
    "    def train(self, sentences, save_loc=None, nr_iter=5, dont_allow=None):\n",
    "        '''Train a model from sentences, and save it at ``save_loc``. ``nr_iter``\n",
    "        controls the number of Perceptron training iterations.\n",
    "        :param sentences: A list of (words, tags) tuples.\n",
    "        :param save_loc: If not ``None``, saves a pickled model in this location.\n",
    "        :param nr_iter: Number of training iterations.\n",
    "        '''\n",
    "        \n",
    "        self._make_tagdict(sentences)\n",
    "        self.model.classes = self.classes\n",
    "        prev, prev2 = self.START\n",
    "        for iter_ in range(nr_iter):\n",
    "            c = 0\n",
    "            n = 0\n",
    "            for sentence in sentences:\n",
    "                words = sentence.words()\n",
    "                tags = sentence.pos_tags()\n",
    "                head = sentence.heads() #indeces\n",
    "                \n",
    "                train_one(2, words, tags, heads)\n",
    "            random.shuffle(sentences)\n",
    "            logging.info(\"Iter {0}: {1}/{2}={3}\".format(iter_, c, n, _pc(c, n)))\n",
    "        self.model.average_weights()\n",
    "        # Pickle as a binary file\n",
    "        if save_loc is not None:\n",
    "            pickle.dump((self.model.weights, self.tagdict, self.classes),\n",
    "                         open(save_loc, 'wb'), -1)\n",
    "        return None\n",
    "\n",
    "    def get_valid_moves(i, n, stack_depth):\n",
    "        moves = []\n",
    "        if i < n:\n",
    "            moves.append(SHIFT)\n",
    "        if stack_depth <= 2:\n",
    "            moves.append(RIGHT)\n",
    "        if stack_depth <= 1:\n",
    "            moves.append(LEFT)\n",
    "        return moves\n",
    "    \n",
    "def extract_features(words, tags, n0, n, stack, parse):\n",
    "    def get_stack_context(depth, stack, data):\n",
    "        if depth >= 3:\n",
    "            return data[stack[-1]], data[stack[-2]], data[stack[-3]]\n",
    "        elif depth >= 2:\n",
    "            return data[stack[-1]], data[stack[-2]], ''\n",
    "        elif depth == 1:\n",
    "            return data[stack[-1]], '', ''\n",
    "        else:\n",
    "            return '', '', ''\n",
    " \n",
    "    def get_buffer_context(i, n, data):\n",
    "        if i + 1 >= n:\n",
    "            return data[i], '', ''\n",
    "        elif i + 2 >= n:\n",
    "            return data[i], data[i + 1], ''\n",
    "        else:\n",
    "            return data[i], data[i + 1], data[i + 2]\n",
    " \n",
    "    def get_parse_context(word, deps, data):\n",
    "        if word == -1:\n",
    "            return 0, '', ''\n",
    "        deps = deps[word]\n",
    "        valency = len(deps)\n",
    "        if not valency:\n",
    "            return 0, '', ''\n",
    "        elif valency == 1:\n",
    "            return 1, data[deps[-1]], ''\n",
    "        else:\n",
    "            return valency, data[deps[-1]], data[deps[-2]]\n",
    " \n",
    "    features = {}\n",
    "    # Set up the context pieces --- the word, W, and tag, T, of:\n",
    "    # S0-2: Top three words on the stack\n",
    "    # N0-2: First three words of the buffer\n",
    "    # n0b1, n0b2: Two leftmost children of the first word of the buffer\n",
    "    # s0b1, s0b2: Two leftmost children of the top word of the stack\n",
    "    # s0f1, s0f2: Two rightmost children of the top word of the stack\n",
    " \n",
    "    depth = len(stack)\n",
    "    s0 = stack[-1] if depth else -1\n",
    " \n",
    "    Ws0, Ws1, Ws2 = get_stack_context(depth, stack, words)\n",
    "    Ts0, Ts1, Ts2 = get_stack_context(depth, stack, tags)\n",
    " \n",
    "    Wn0, Wn1, Wn2 = get_buffer_context(n0, n, words)\n",
    "    Tn0, Tn1, Tn2 = get_buffer_context(n0, n, tags)\n",
    " \n",
    "    Vn0b, Wn0b1, Wn0b2 = get_parse_context(n0, parse.lefts, words)\n",
    "    Vn0b, Tn0b1, Tn0b2 = get_parse_context(n0, parse.lefts, tags)\n",
    " \n",
    "    Vn0f, Wn0f1, Wn0f2 = get_parse_context(n0, parse.rights, words)\n",
    "    _, Tn0f1, Tn0f2 = get_parse_context(n0, parse.rights, tags)\n",
    " \n",
    "    Vs0b, Ws0b1, Ws0b2 = get_parse_context(s0, parse.lefts, words)\n",
    "    _, Ts0b1, Ts0b2 = get_parse_context(s0, parse.lefts, tags)\n",
    " \n",
    "    Vs0f, Ws0f1, Ws0f2 = get_parse_context(s0, parse.rights, words)\n",
    "    _, Ts0f1, Ts0f2 = get_parse_context(s0, parse.rights, tags)\n",
    " \n",
    "    # Cap numeric features at 5? \n",
    "    # String-distance\n",
    "    Ds0n0 = min((n0 - s0, 5)) if s0 != 0 else 0\n",
    " \n",
    "    features['bias'] = 1\n",
    "    # Add word and tag unigrams\n",
    "    for w in (Wn0, Wn1, Wn2, Ws0, Ws1, Ws2, Wn0b1, Wn0b2, Ws0b1, Ws0b2, Ws0f1, Ws0f2):\n",
    "        if w:\n",
    "            features['w=%s' % w] = 1\n",
    "    for t in (Tn0, Tn1, Tn2, Ts0, Ts1, Ts2, Tn0b1, Tn0b2, Ts0b1, Ts0b2, Ts0f1, Ts0f2):\n",
    "        if t:\n",
    "            features['t=%s' % t] = 1\n",
    " \n",
    "    # Add word/tag pairs\n",
    "    for i, (w, t) in enumerate(((Wn0, Tn0), (Wn1, Tn1), (Wn2, Tn2), (Ws0, Ts0))):\n",
    "        if w or t:\n",
    "            features['%d w=%s, t=%s' % (i, w, t)] = 1\n",
    " \n",
    "    # Add some bigrams\n",
    "    features['s0w=%s,  n0w=%s' % (Ws0, Wn0)] = 1\n",
    "    features['wn0tn0-ws0 %s/%s %s' % (Wn0, Tn0, Ws0)] = 1\n",
    "    features['wn0tn0-ts0 %s/%s %s' % (Wn0, Tn0, Ts0)] = 1\n",
    "    features['ws0ts0-wn0 %s/%s %s' % (Ws0, Ts0, Wn0)] = 1\n",
    "    features['ws0-ts0 tn0 %s/%s %s' % (Ws0, Ts0, Tn0)] = 1\n",
    "    features['wt-wt %s/%s %s/%s' % (Ws0, Ts0, Wn0, Tn0)] = 1\n",
    "    features['tt s0=%s n0=%s' % (Ts0, Tn0)] = 1\n",
    "    features['tt n0=%s n1=%s' % (Tn0, Tn1)] = 1\n",
    " \n",
    "    # Add some tag trigrams\n",
    "    trigrams = ((Tn0, Tn1, Tn2), (Ts0, Tn0, Tn1), (Ts0, Ts1, Tn0), \n",
    "                (Ts0, Ts0f1, Tn0), (Ts0, Ts0f1, Tn0), (Ts0, Tn0, Tn0b1),\n",
    "                (Ts0, Ts0b1, Ts0b2), (Ts0, Ts0f1, Ts0f2), (Tn0, Tn0b1, Tn0b2),\n",
    "                (Ts0, Ts1, Ts1))\n",
    "    for i, (t1, t2, t3) in enumerate(trigrams):\n",
    "        if t1 or t2 or t3:\n",
    "            features['ttt-%d %s %s %s' % (i, t1, t2, t3)] = 1\n",
    " \n",
    "    # Add some valency and distance features\n",
    "    vw = ((Ws0, Vs0f), (Ws0, Vs0b), (Wn0, Vn0b))\n",
    "    vt = ((Ts0, Vs0f), (Ts0, Vs0b), (Tn0, Vn0b))\n",
    "    d = ((Ws0, Ds0n0), (Wn0, Ds0n0), (Ts0, Ds0n0), (Tn0, Ds0n0),\n",
    "        ('t' + Tn0+Ts0, Ds0n0), ('w' + Wn0+Ws0, Ds0n0))\n",
    "    for i, (w_t, v_d) in enumerate(vw + vt + d):\n",
    "        if w_t or v_d:\n",
    "            features['val/d-%d %s %d' % (i, w_t, v_d)] = 1\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import codecs\n",
    "#### get training set from UD\n",
    "def load_tagged_sentences(file_name):\n",
    "    sentences_w_tags = []\n",
    "    count = 0\n",
    "    words=[]\n",
    "    tags=[]\n",
    "    on_sentence = False\n",
    "    for line in codecs.open(file_name, 'r', encoding=\"utf-8\"):\n",
    "    \n",
    "        vals = line.split('\\t')\n",
    "        if (len(vals) > 1):\n",
    "            on_sentence = True\n",
    "            words.append(vals[1])\n",
    "            tags.append(vals[3])\n",
    "        elif (on_sentence):\n",
    "            on_sentence=False\n",
    "            sentences_w_tags.append((words, tags))\n",
    "            words=[]\n",
    "            tags=[]\n",
    "    \n",
    "    return sentences_w_tags # [ ([\"word\", \"word\", \"word\"], [\"tag\", \"tag\", \"tag\"]), next sentece...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "parser = Spacy_Parser() \n",
    "eng_train = \"../../Data/UD_English/en-ud-train.conllu\"\n",
    "\n",
    "tagged_data = load_tagged_sentences(eng_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "parser.train_tagger(tagged_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('these', u'PRON', 5.128999999999998),\n",
       "  ('are', u'VERB', 21.255000000000006),\n",
       "  ('some', u'DET', 11.432000000000002),\n",
       "  ('words', u'NOUN', 22.342000000000006),\n",
       "  ('to', u'PART', 22.369999999999997),\n",
       "  ('be', u'AUX', 8.712),\n",
       "  ('tagged', u'VERB', 19.621000000000002),\n",
       "  ('.', u'PUNCT', 28.938)],\n",
       " [('a', u'DET', 1.2759999999999927)]]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parser.tagger.tag(\" \".join([\"these\", \"are\", \"some\", \"words\", \"to\", \"be\", \"tagged\", \".\",\"\\n\",\"a\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[], [], [], [], []]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
