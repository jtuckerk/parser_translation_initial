{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Averaged perceptron classifier. Implementation geared for simplicity rather than\n",
    "efficiency.\n",
    "\"\"\"\n",
    "#from __future__ import absolute_import\n",
    "import os\n",
    "import random\n",
    "from collections import defaultdict\n",
    "from collections import defaultdict\n",
    "import pickle\n",
    "import random\n",
    "\n",
    "import logging\n",
    "\n",
    "from textblob.base import BaseTagger\n",
    "from textblob.tokenizers import WordTokenizer, SentenceTokenizer\n",
    "from textblob.exceptions import MissingCorpusError\n",
    "\n",
    "from nlp_jtk import Token, Sentence\n",
    "\n",
    "class AveragedPerceptron(object):\n",
    "\n",
    "    '''An averaged perceptron, as implemented by Matthew Honnibal.\n",
    "    See more implementation details here:\n",
    "        http://honnibal.wordpress.com/2013/09/11/a-good-part-of-speechpos-tagger-in-about-200-lines-of-python/\n",
    "    '''\n",
    "\n",
    "    def __init__(self):\n",
    "        # Each feature gets its own weight vector, so weights is a dict-of-dicts\n",
    "        \n",
    "        self.weights = {}\n",
    "        self.classes = set()\n",
    "        # The accumulated values, for the averaging. These will be keyed by\n",
    "        # feature/clas tuples\n",
    "        self._totals = defaultdict(int)\n",
    "        # The last time the feature was changed, for the averaging. Also\n",
    "        # keyed by feature/clas tuples\n",
    "        # (tstamps is short for timestamps)\n",
    "        self._tstamps = defaultdict(int)\n",
    "        # Number of instances seen\n",
    "        self.i = 0\n",
    "\n",
    "    def predict(self, features, dont_allow):\n",
    "        '''Dot-product the features and current weights and return the best label.'''\n",
    "        scores = defaultdict(float)\n",
    "        \n",
    "        for feat, value in features.items():\n",
    "            \n",
    "            if feat not in self.weights or value == 0:\n",
    "                continue\n",
    "            weights = self.weights[feat]\n",
    "            for label, weight in weights.items():\n",
    "                scores[label] += value * weight\n",
    "        # Do a secondary alphabetic sort, for stability\n",
    "        sort_by_score = lambda d: (d[1], d)\n",
    "        \n",
    "        first_found=False\n",
    "        maxClass = \"None\"\n",
    "        maxScore = 0\n",
    "    \n",
    "        secondMaxClass = \"None\"\n",
    "        secondMaxScore = 0\n",
    "        \n",
    "        for label, score in sorted(scores.iteritems(), key=sort_by_score, reverse=True):\n",
    "            if(label != dont_allow and not first_found):\n",
    "                maxClass = label\n",
    "                maxScore = score\n",
    "                first_found=True\n",
    "            elif(label != dont_allow and first_found):\n",
    "                secondMaxClass = label\n",
    "                secondMaxScore = score\n",
    "                break\n",
    "      \n",
    "        return maxClass, maxScore-secondMaxScore\n",
    "    \n",
    "    def score(self, features):\n",
    "        all_weights = self.weights\n",
    "        for clas in self.classes:\n",
    "            print clas\n",
    "        scores = dict((clas, 0) for clas in self.classes)\n",
    "        for feat, value in features.items():\n",
    "            if value == 0:\n",
    "                continue\n",
    "            if feat not in all_weights:\n",
    "                continue\n",
    "            weights = all_weights[feat]\n",
    "            for clas, weight in weights.items():\n",
    "                scores[clas] += value * weight\n",
    "        return scores\n",
    "\n",
    "    def update(self, truth, guess, features):\n",
    "        '''Update the feature weights.'''\n",
    "        def upd_feat(c, f, w, v):\n",
    "            param = (f, c)\n",
    "            self._totals[param] += (self.i - self._tstamps[param]) * w\n",
    "            self._tstamps[param] = self.i\n",
    "            self.weights[f][c] = w + v\n",
    "\n",
    "        self.i += 1\n",
    "        if truth == guess:\n",
    "            return None\n",
    "        for f in features:\n",
    "            weights = self.weights.setdefault(f, {})\n",
    "            upd_feat(truth, f, weights.get(truth, 0.0), 1.0)\n",
    "            upd_feat(guess, f, weights.get(guess, 0.0), -1.0)\n",
    "        return None\n",
    "\n",
    "    def average_weights(self):\n",
    "        '''Average weights from all iterations.'''\n",
    "        for feat, weights in self.weights.items():\n",
    "            new_feat_weights = {}\n",
    "            for clas, weight in weights.items():\n",
    "                param = (feat, clas)\n",
    "                total = self._totals[param]\n",
    "                total += (self.i - self._tstamps[param]) * weight\n",
    "                averaged = round(total / float(self.i), 3)\n",
    "                if averaged:\n",
    "                    new_feat_weights[clas] = averaged\n",
    "            self.weights[feat] = new_feat_weights\n",
    "        return None\n",
    "\n",
    "    def save(self, path):\n",
    "        '''Save the pickled model weights.'''\n",
    "        return pickle.dump(dict(self.weights), open(path, 'w'))\n",
    "\n",
    "    def load(self, path):\n",
    "        '''Load the pickled model weights.'''\n",
    "        self.weights = pickle.load(open(path))\n",
    "        return None\n",
    "\n",
    "\n",
    "def train(nr_iter, examples):\n",
    "    '''Return an averaged perceptron model trained on ``examples`` for\n",
    "    ``nr_iter`` iterations.\n",
    "    '''\n",
    "    model = AveragedPerceptron()\n",
    "    for i in range(nr_iter):\n",
    "        random.shuffle(examples)\n",
    "        for features, class_ in examples:\n",
    "            scores = model.predict(features)\n",
    "            guess, score = max(scores.items(), key=lambda i: i[1])\n",
    "            if guess != class_:\n",
    "                model.update(class_, guess, features)\n",
    "    model.average_weights()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "PICKLE = \"trontagger-0.1.0.pickle\"\n",
    "class PerceptronTagger(AveragedPerceptron):\n",
    "\n",
    "    '''Greedy Averaged Perceptron tagger, as implemented by Matthew Honnibal.\n",
    "    See more implementation details here:\n",
    "        http://honnibal.wordpress.com/2013/09/11/a-good-part-of-speechpos-tagger-in-about-200-lines-of-python/\n",
    "    :param load: Load the pickled model upon instantiation.\n",
    "    '''\n",
    "\n",
    "    START = ['-START-', '-START2-']\n",
    "    END = ['-END-', '-END2-']\n",
    "    #AP_MODEL_LOC = os.path.join(os.path.dirname(__file__), PICKLE)\n",
    "\n",
    "    def __init__(self, load=False):\n",
    "        self.model = AveragedPerceptron()\n",
    "        self.tagdict = {}\n",
    "        self.classes = set()\n",
    "        if load:\n",
    "            self.load(self.AP_MODEL_LOC)\n",
    "\n",
    "    def tag(self, corpus, tokenize=True, dont_allow=None):\n",
    "        '''Tags a string `corpus`.'''\n",
    "        # Assume untokenized corpus has \\n between sentences and ' ' between words\n",
    "        s_split = SentenceTokenizer().tokenize if tokenize else lambda t: t.split('\\n')\n",
    "        w_split = WordTokenizer().tokenize if tokenize else lambda s: s.split()\n",
    "        def split_sents(corpus):\n",
    "            for s in s_split(corpus):\n",
    "                yield w_split(s)\n",
    "\n",
    "        prev, prev2 = self.START\n",
    "        tagged_sentence = Sentence()\n",
    "        sentences = []\n",
    "        for words in split_sents(corpus):\n",
    "            tagged_sentence = Sentence()\n",
    "          \n",
    "            context = self.START + [self._normalize(w) for w in words] + self.END\n",
    "            for i, word in enumerate(words):\n",
    "                tag = None#self.tagdict.get(word)\n",
    "                confidence = 30\n",
    "                if not tag:\n",
    "                    features = self._get_features(i, word, context, prev, prev2)\n",
    "                    tag, confidence = self.model.predict(features, dont_allow)\n",
    "                tagged_sentence.token_list.append(Token(word,tag,'--',confidence))\n",
    "                \n",
    "                prev2 = prev\n",
    "                prev = tag\n",
    "            sentences.append(tagged_sentence)\n",
    "        return sentences\n",
    "\n",
    "    def train(self, sentences, save_loc=None, nr_iter=5, dont_allow=None):\n",
    "        '''Train a model from sentences, and save it at ``save_loc``. ``nr_iter``\n",
    "        controls the number of Perceptron training iterations.\n",
    "        :param sentences: A list of (words, tags) tuples.\n",
    "        :param save_loc: If not ``None``, saves a pickled model in this location.\n",
    "        :param nr_iter: Number of training iterations.\n",
    "        '''\n",
    "        \"Hi train\"\n",
    "        self._make_tagdict(sentences)\n",
    "        self.model.classes = self.classes\n",
    "        prev, prev2 = self.START\n",
    "        for iter_ in range(nr_iter):\n",
    "            c = 0\n",
    "            n = 0\n",
    "            for sentence in sentences:\n",
    "                \n",
    "                words = sentence.words()\n",
    "                tags = sentence.pos_tags()\n",
    "                context = self.START + [self._normalize(w) for w in words] \\\n",
    "                                                                    + self.END\n",
    "                for i, word in enumerate(words):\n",
    "                    guess = None # self.tagdict.get(word)\n",
    "                    confidence = 30\n",
    "                    if not guess:\n",
    "                        feats = self._get_features(i, word, context, prev, prev2)\n",
    "                        guess, confidence = self.model.predict(feats, dont_allow)\n",
    "                        self.model.update(tags[i], guess, feats)\n",
    "                    prev2 = prev\n",
    "                    prev = guess\n",
    "                    c += guess == tags[i]\n",
    "                    n += 1\n",
    "            random.shuffle(sentences)\n",
    "            logging.info(\"Iter {0}: {1}/{2}={3}\".format(iter_, c, n, _pc(c, n)))\n",
    "        self.model.average_weights()\n",
    "        # Pickle as a binary file\n",
    "        if save_loc is not None:\n",
    "            pickle.dump((self.model.weights, self.tagdict, self.classes),\n",
    "                         open(save_loc, 'wb'), -1)\n",
    "        return None\n",
    "\n",
    "    def load(self, loc):\n",
    "        print '''Load a pickled model.'''\n",
    "        try:\n",
    "            w_td_c = pickle.load(open(loc, 'rb'))\n",
    "        except IOError:\n",
    "            msg = (\"Missing trontagger.pickle file.\")\n",
    "            raise MissingCorpusError(msg)\n",
    "        self.model.weights, self.tagdict, self.classes = w_td_c\n",
    "        self.model.classes = self.classes\n",
    "        return None\n",
    "\n",
    "    def _normalize(self, word):\n",
    "        '''Normalization used in pre-processing.\n",
    "        - All words are lower cased\n",
    "        - Digits in the range 1800-2100 are represented as !YEAR;\n",
    "        - Other digits are represented as !DIGITS\n",
    "        :rtype: str\n",
    "        '''\n",
    "        if '-' in word and word[0] != '-':\n",
    "            return '!HYPHEN'\n",
    "        elif word.isdigit() and len(word) == 4:\n",
    "            return '!YEAR'\n",
    "        elif word[0].isdigit():\n",
    "            return '!DIGITS'\n",
    "        else:\n",
    "            return word #.lower()\n",
    "\n",
    "    def _get_features(self, i, word, context, prev, prev2):\n",
    "        '''Map tokens into a feature representation, implemented as a\n",
    "        {hashable: float} dict. If the features change, a new model must be\n",
    "        trained.\n",
    "        '''\n",
    "        def add(name, *args):\n",
    "            features[' '.join((name,) + tuple(args))] += 1\n",
    "\n",
    "        i += len(self.START)\n",
    "        features = defaultdict(int)\n",
    "        # It's useful to have a constant feature, which acts sort of like a prior\n",
    "        add('bias')\n",
    "        add('i suffix', word[-3:])\n",
    "        add('i pref1', word[0])\n",
    "        add('i-1 tag', prev)\n",
    "        add('i-2 tag', prev2)\n",
    "        add('i tag+i-2 tag', prev, prev2)\n",
    "        add('i word', context[i])\n",
    "        add('i-1 tag+i word', prev, context[i])\n",
    "        add('i-1 word', context[i-1])\n",
    "        add('i-1 suffix', context[i-1][-3:])\n",
    "        add('i-2 word', context[i-2])\n",
    "        add('i+1 word', context[i+1])\n",
    "        add('i+1 suffix', context[i+1][-3:])\n",
    "        add('i+2 word', context[i+2])\n",
    "        return features\n",
    "\n",
    "    def _make_tagdict(self, sentences):\n",
    "        \n",
    "        '''Make a tag dictionary for single-tag words.'''\n",
    "        counts = defaultdict(lambda: defaultdict(int))\n",
    "\n",
    "        for sentence in sentences:\n",
    "            words = sentence.words()\n",
    "            tags = sentence.pos_tags()\n",
    "            \n",
    "            for word, tag in zip(words, tags):\n",
    "                counts[word][tag] += 1\n",
    "\n",
    "                self.classes.add(tag)\n",
    "        freq_thresh = 20\n",
    "        ambiguity_thresh = 0.97\n",
    "        for word, tag_freqs in counts.items():\n",
    "            tag, mode = max(tag_freqs.items(), key=lambda item: item[1])\n",
    "            n = sum(tag_freqs.values())\n",
    "            # Don't add rare words to the tag dictionary\n",
    "            # Only add quite unambiguous words\n",
    "            if n >= freq_thresh and (float(mode) / n) >= ambiguity_thresh:\n",
    "                self.tagdict[word] = tag\n",
    "\n",
    "\n",
    "def _pc(n, d):\n",
    "    return (float(n) / d) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val = [\"d\",\"a\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = [\"a\", \"b\", \"c\", \"d\"]\n",
    "ad = dict((clas,0) for clas in a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ad['c']=4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "next_move = max(val, key=lambda move: ad[move])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next_move"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"A simple implementation of a greedy transition-based parser. Released under BSD license.\"\"\"\n",
    "from os import path\n",
    "import os\n",
    "import sys\n",
    "from collections import defaultdict\n",
    "import random\n",
    "import time\n",
    "import pickle\n",
    "\n",
    "SHIFT = 0; RIGHT = 1; LEFT = 2;\n",
    "MOVES = (SHIFT, RIGHT, LEFT)\n",
    "START = ['-START-', '-START2-']\n",
    "END = ['-END-', '-END2-']\n",
    "\n",
    "\n",
    "class DefaultList(list):\n",
    "    \"\"\"A list that returns a default value if index out of bounds.\"\"\"\n",
    "    def __init__(self, default=None):\n",
    "        self.default = default\n",
    "        list.__init__(self)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        try:\n",
    "            return list.__getitem__(self, index)\n",
    "        except IndexError:\n",
    "            return self.default\n",
    "\n",
    "\n",
    "class Parse(object):\n",
    "    def __init__(self, n):\n",
    "        self.n = n\n",
    "        self.heads = [None] * (n-1)\n",
    "        self.labels = [None] * (n-1)\n",
    "        self.lefts = []\n",
    "        self.rights = []\n",
    "        for i in range(n+1):\n",
    "            self.lefts.append(DefaultList(0))\n",
    "            self.rights.append(DefaultList(0))\n",
    "\n",
    "    def add(self, head, child, label=None):\n",
    "        self.heads[child] = head\n",
    "        self.labels[child] = label\n",
    "        if child < head:\n",
    "            self.lefts[head].append(child)\n",
    "        else:\n",
    "            self.rights[head].append(child)\n",
    "\n",
    "\n",
    "class Parser(object):\n",
    "    def __init__(self, load=False):\n",
    "        model_dir = os.path.dirname(__file__)\n",
    "        self.model = Perceptron(MOVES)\n",
    "        if load:\n",
    "            self.model.load(path.join(model_dir, 'parser.pickle'))\n",
    "        self.tagger = PerceptronTagger(load=load)\n",
    "        self.confusion_matrix = defaultdict(lambda: defaultdict(int))\n",
    "\n",
    "    def save(self):\n",
    "        self.model.save(path.join(os.path.dirname(__file__), 'parser.pickle'))\n",
    "        self.tagger.save()\n",
    "    \n",
    "    def parse(self, corpus):\n",
    "        sentences = self.tagger.tag(corpus)\n",
    "        for sentence in sentences:\n",
    "            words = sentence.words()\n",
    "            tags = sentence.pos_tags()\n",
    "            n = len(words)\n",
    "            i = 2; stack = [1]; parse = Parse(n)\n",
    "       \n",
    "            while stack or (i+1) < n:\n",
    "                features = extract_features(words, tags, i, n, stack, parse)\n",
    "                scores = self.model.score(features)\n",
    "                valid_moves = get_valid_moves(i, n, len(stack))\n",
    "                guess = max(valid_moves, key=lambda move: scores[move])\n",
    "                i = transition(guess, i, stack, parse)\n",
    "                \n",
    "            sentence.set_heads(parse.heads)\n",
    "        return sentences\n",
    "\n",
    "    def train_one(self, itn, words, gold_tags, gold_heads):\n",
    "        n = len(words)\n",
    "        i = 2; stack = [1]; parse = Parse(n)\n",
    "        tags = self.tagger.tag_one(words) # todo may need to update tagger\n",
    "        while stack or (i + 1) < n:\n",
    "            features = extract_features(words, tags, i, n, stack, parse)\n",
    "            scores = self.model.score(features)\n",
    "            valid_moves = get_valid_moves(i, n, len(stack))\n",
    "            gold_moves = get_gold_moves(i, n, stack, parse.heads, gold_heads)\n",
    "            guess = max(valid_moves, key=lambda move: scores[move])\n",
    "            assert gold_moves\n",
    "            best = max(gold_moves, key=lambda move: scores[move])\n",
    "            self.model.update(best, guess, features)\n",
    "            i = transition(guess, i, stack, parse)\n",
    "            self.confusion_matrix[best][guess] += 1\n",
    "        return len([i for i in range(n-1) if parse.heads[i] == gold_heads[i]])\n",
    "\n",
    "\n",
    "def transition(move, i, stack, parse):\n",
    "    if move == SHIFT:\n",
    "        stack.append(i)\n",
    "        return i + 1\n",
    "    elif move == RIGHT:\n",
    "        parse.add(stack[-2], stack.pop())\n",
    "        return i\n",
    "    elif move == LEFT:\n",
    "        parse.add(i, stack.pop())\n",
    "        return i\n",
    "    assert move in MOVES\n",
    "\n",
    "\n",
    "def get_valid_moves(i, n, stack_depth):\n",
    "    moves = []\n",
    "    if (i+1) < n:\n",
    "        moves.append(SHIFT)\n",
    "    if stack_depth >= 2:\n",
    "        moves.append(RIGHT)\n",
    "    if stack_depth >= 1:\n",
    "        moves.append(LEFT)\n",
    "    return moves\n",
    "\n",
    "\n",
    "def get_gold_moves(n0, n, stack, heads, gold):\n",
    "    def deps_between(target, others, gold):\n",
    "        for word in others:\n",
    "            if gold[word] == target or gold[target] == word:\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "    valid = get_valid_moves(n0, n, len(stack))\n",
    "    if not stack or (SHIFT in valid and gold[n0] == stack[-1]):\n",
    "        return [SHIFT]\n",
    "    if gold[stack[-1]] == n0:\n",
    "        return [LEFT]\n",
    "    costly = set([m for m in MOVES if m not in valid])\n",
    "    # If the word behind s0 is its gold head, Left is incorrect\n",
    "    if len(stack) >= 2 and gold[stack[-1]] == stack[-2]:\n",
    "        costly.add(LEFT)\n",
    "    # If there are any dependencies between n0 and the stack,\n",
    "    # pushing n0 will lose them.\n",
    "    if SHIFT not in costly and deps_between(n0, stack, gold):\n",
    "        costly.add(SHIFT)\n",
    "    # If there are any dependencies between s0 and the buffer, popping\n",
    "    # s0 will lose them.\n",
    "    if deps_between(stack[-1], range(n0+1, n-1), gold):\n",
    "        costly.add(LEFT)\n",
    "        costly.add(RIGHT)\n",
    "    return [m for m in MOVES if m not in costly]\n",
    "\n",
    "\n",
    "def extract_features(words, tags, n0, n, stack, parse):\n",
    "    def get_stack_context(depth, stack, data):\n",
    "        if depth >= 3:\n",
    "            return data[stack[-1]], data[stack[-2]], data[stack[-3]]\n",
    "        elif depth >= 2:\n",
    "            return data[stack[-1]], data[stack[-2]], ''\n",
    "        elif depth == 1:\n",
    "            return data[stack[-1]], '', ''\n",
    "        else:\n",
    "            return '', '', ''\n",
    "\n",
    "    def get_buffer_context(i, n, data):\n",
    "        if i + 1 >= n:\n",
    "            return data[i], '', ''\n",
    "        elif i + 2 >= n:\n",
    "            return data[i], data[i + 1], ''\n",
    "        else:\n",
    "            return data[i], data[i + 1], data[i + 2]\n",
    "\n",
    "    def get_parse_context(word, deps, data):\n",
    "        if word == -1:\n",
    "            return 0, '', ''\n",
    "        deps = deps[word]\n",
    "        valency = len(deps)\n",
    "        if not valency:\n",
    "            return 0, '', ''\n",
    "        elif valency == 1:\n",
    "            return 1, data[deps[-1]], ''\n",
    "        else:\n",
    "            return valency, data[deps[-1]], data[deps[-2]]\n",
    "\n",
    "    features = {}\n",
    "    # Set up the context pieces --- the word (W) and tag (T) of:\n",
    "    # S0-2: Top three words on the stack\n",
    "    # N0-2: First three words of the buffer\n",
    "    # n0b1, n0b2: Two leftmost children of the first word of the buffer\n",
    "    # s0b1, s0b2: Two leftmost children of the top word of the stack\n",
    "    # s0f1, s0f2: Two rightmost children of the top word of the stack\n",
    "\n",
    "    depth = len(stack)\n",
    "    s0 = stack[-1] if depth else -1\n",
    "\n",
    "    Ws0, Ws1, Ws2 = get_stack_context(depth, stack, words)\n",
    "    Ts0, Ts1, Ts2 = get_stack_context(depth, stack, tags)\n",
    "   \n",
    "    Wn0, Wn1, Wn2 = get_buffer_context(n0, n, words)\n",
    "    Tn0, Tn1, Tn2 = get_buffer_context(n0, n, tags)\n",
    "    \n",
    "    Vn0b, Wn0b1, Wn0b2 = get_parse_context(n0, parse.lefts, words)\n",
    "    Vn0b, Tn0b1, Tn0b2 = get_parse_context(n0, parse.lefts, tags)\n",
    "    \n",
    "    Vn0f, Wn0f1, Wn0f2 = get_parse_context(n0, parse.rights, words)\n",
    "    _, Tn0f1, Tn0f2 = get_parse_context(n0, parse.rights, tags)\n",
    "  \n",
    "    Vs0b, Ws0b1, Ws0b2 = get_parse_context(s0, parse.lefts, words)\n",
    "    _, Ts0b1, Ts0b2 = get_parse_context(s0, parse.lefts, tags)\n",
    "\n",
    "    Vs0f, Ws0f1, Ws0f2 = get_parse_context(s0, parse.rights, words)\n",
    "    _, Ts0f1, Ts0f2 = get_parse_context(s0, parse.rights, tags)\n",
    "    \n",
    "    # Cap numeric features at 5? \n",
    "    # String-distance\n",
    "    Ds0n0 = min((n0 - s0, 5)) if s0 != 0 else 0\n",
    "\n",
    "    features['bias'] = 1\n",
    "    # Add word and tag unigrams\n",
    "    for w in (Wn0, Wn1, Wn2, Ws0, Ws1, Ws2, Wn0b1, Wn0b2, Ws0b1, Ws0b2, Ws0f1, Ws0f2):\n",
    "        if w:\n",
    "            features['w=%s' % w] = 1\n",
    "    for t in (Tn0, Tn1, Tn2, Ts0, Ts1, Ts2, Tn0b1, Tn0b2, Ts0b1, Ts0b2, Ts0f1, Ts0f2):\n",
    "        if t:\n",
    "            features['t=%s' % t] = 1\n",
    "\n",
    "    # Add word/tag pairs\n",
    "    for i, (w, t) in enumerate(((Wn0, Tn0), (Wn1, Tn1), (Wn2, Tn2), (Ws0, Ts0))):\n",
    "        if w or t:\n",
    "            features['%d w=%s, t=%s' % (i, w, t)] = 1\n",
    "\n",
    "    # Add some bigrams\n",
    "    features['s0w=%s,  n0w=%s' % (Ws0, Wn0)] = 1\n",
    "    features['wn0tn0-ws0 %s/%s %s' % (Wn0, Tn0, Ws0)] = 1\n",
    "    features['wn0tn0-ts0 %s/%s %s' % (Wn0, Tn0, Ts0)] = 1\n",
    "    features['ws0ts0-wn0 %s/%s %s' % (Ws0, Ts0, Wn0)] = 1\n",
    "    features['ws0-ts0 tn0 %s/%s %s' % (Ws0, Ts0, Tn0)] = 1\n",
    "    features['wt-wt %s/%s %s/%s' % (Ws0, Ts0, Wn0, Tn0)] = 1\n",
    "    features['tt s0=%s n0=%s' % (Ts0, Tn0)] = 1\n",
    "    features['tt n0=%s n1=%s' % (Tn0, Tn1)] = 1\n",
    "\n",
    "    # Add some tag trigrams\n",
    "    trigrams = ((Tn0, Tn1, Tn2), (Ts0, Tn0, Tn1), (Ts0, Ts1, Tn0), \n",
    "                (Ts0, Ts0f1, Tn0), (Ts0, Ts0f1, Tn0), (Ts0, Tn0, Tn0b1),\n",
    "                (Ts0, Ts0b1, Ts0b2), (Ts0, Ts0f1, Ts0f2), (Tn0, Tn0b1, Tn0b2),\n",
    "                (Ts0, Ts1, Ts1))\n",
    "    for i, (t1, t2, t3) in enumerate(trigrams):\n",
    "        if t1 or t2 or t3:\n",
    "            features['ttt-%d %s %s %s' % (i, t1, t2, t3)] = 1\n",
    "\n",
    "    # Add some valency and distance features\n",
    "    vw = ((Ws0, Vs0f), (Ws0, Vs0b), (Wn0, Vn0b))\n",
    "    vt = ((Ts0, Vs0f), (Ts0, Vs0b), (Tn0, Vn0b))\n",
    "    d = ((Ws0, Ds0n0), (Wn0, Ds0n0), (Ts0, Ds0n0), (Tn0, Ds0n0),\n",
    "         ('t' + Tn0+Ts0, Ds0n0), ('w' + Wn0+Ws0, Ds0n0))\n",
    "    for i, (w_t, v_d) in enumerate(vw + vt + d):\n",
    "        if w_t or v_d:\n",
    "            features['val/d-%d %s %d' % (i, w_t, v_d)] = 1\n",
    "    return features\n",
    "\n",
    "\n",
    "class Perceptron(object):\n",
    "    def __init__(self, classes=None):\n",
    "        # Each feature gets its own weight vector, so weights is a dict-of-arrays\n",
    "        self.classes = classes\n",
    "        self.weights = {}\n",
    "        # The accumulated values, for the averaging. These will be keyed by\n",
    "        # feature/clas tuples\n",
    "        self._totals = defaultdict(int)\n",
    "        # The last time the feature was changed, for the averaging. Also\n",
    "        # keyed by feature/clas tuples\n",
    "        # (tstamps is short for timestamps)\n",
    "        self._tstamps = defaultdict(int)\n",
    "        # Number of instances seen\n",
    "        self.i = 0\n",
    "\n",
    "    def predict(self, features):\n",
    "        '''Dot-product the features and current weights and return the best class.'''\n",
    "        scores = self.score(features)\n",
    "        # Do a secondary alphabetic sort, for stability\n",
    "        return max(self.classes, key=lambda clas: (scores[clas], clas))\n",
    "\n",
    "    def score(self, features):\n",
    "        all_weights = self.weights\n",
    "        scores = dict((clas, 0) for clas in self.classes)\n",
    "        for feat, value in features.items():\n",
    "            if value == 0:\n",
    "                continue\n",
    "            if feat not in all_weights:\n",
    "                continue\n",
    "            weights = all_weights[feat]\n",
    "            for clas, weight in weights.items():\n",
    "                scores[clas] += value * weight\n",
    "        return scores\n",
    "\n",
    "    def update(self, truth, guess, features):       \n",
    "        def upd_feat(c, f, w, v):\n",
    "            param = (f, c)\n",
    "            self._totals[param] += (self.i - self._tstamps[param]) * w\n",
    "            self._tstamps[param] = self.i\n",
    "            self.weights[f][c] = w + v\n",
    "\n",
    "        self.i += 1\n",
    "        if truth == guess:\n",
    "            return None\n",
    "        for f in features:\n",
    "            weights = self.weights.setdefault(f, {})\n",
    "            upd_feat(truth, f, weights.get(truth, 0.0), 1.0)\n",
    "            upd_feat(guess, f, weights.get(guess, 0.0), -1.0)\n",
    "\n",
    "    def average_weights(self):\n",
    "        for feat, weights in self.weights.items():\n",
    "            new_feat_weights = {}\n",
    "            for clas, weight in weights.items():\n",
    "                param = (feat, clas)\n",
    "                total = self._totals[param]\n",
    "                total += (self.i - self._tstamps[param]) * weight\n",
    "                averaged = round(total / float(self.i), 3)\n",
    "                if averaged:\n",
    "                    new_feat_weights[clas] = averaged\n",
    "            self.weights[feat] = new_feat_weights\n",
    "\n",
    "    def save(self, path):\n",
    "        print \"Saving model to %s\" % path\n",
    "        pickle.dump(self.weights, open(path, 'w'))\n",
    "\n",
    "    def load(self, path):\n",
    "        self.weights = pickle.load(open(path))\n",
    "\n",
    "\n",
    "class PerceptronTagger(object):\n",
    "    '''Greedy Averaged Perceptron tagger'''\n",
    "    model_loc = os.path.join(os.path.dirname(__file__), 'tagger.pickle')\n",
    "    def __init__(self, classes=None, load=True):\n",
    "        self.tagdict = {}\n",
    "        if classes:\n",
    "            self.classes = classes\n",
    "        else:\n",
    "            self.classes = set()\n",
    "        self.model = Perceptron(self.classes)\n",
    "        if load:\n",
    "            self.load(PerceptronTagger.model_loc)\n",
    "\n",
    "    def tag_one(self, words, tokenize=False):\n",
    "        prev, prev2 = START\n",
    "        tags = DefaultList('') \n",
    "        context = START + [self._normalize(w) for w in words] + END\n",
    "        for i, word in enumerate(words):\n",
    "            tag = self.tagdict.get(word)\n",
    "            if not tag:\n",
    "                features = self._get_features(i, word, context, prev, prev2)\n",
    "                tag = self.model.predict(features)\n",
    "            tags.append(tag)\n",
    "            prev2 = prev; prev = tag\n",
    "        return tags\n",
    "\n",
    "    def tag(self, corpus):\n",
    "        print \"implement me please\"\n",
    "        \n",
    "    def start_training(self, sentences):\n",
    "        self._make_tagdict(sentences)\n",
    "        self.model = Perceptron(self.classes)\n",
    "\n",
    "    def train(self, sentences, save_loc=None, nr_iter=5):\n",
    "        '''Train a model from sentences, and save it at save_loc. nr_iter\n",
    "        controls the number of Perceptron training iterations.'''\n",
    "        self.start_training(sentences)\n",
    "        for iter_ in range(nr_iter):\n",
    "            for sent in sentences:\n",
    "                words = sent.words()\n",
    "                tags = sent.pos_tags()\n",
    "                self.train_one(words, tags)\n",
    "            random.shuffle(sentences)\n",
    "        self.end_training(save_loc)\n",
    "\n",
    "    def save(self):\n",
    "        # Pickle as a binary file\n",
    "        pickle.dump((self.model.weights, self.tagdict, self.classes),\n",
    "                    open(PerceptronTagger.model_loc, 'wb'), -1)\n",
    "\n",
    "    def train_one(self, words, tags):\n",
    "        prev, prev2 = START\n",
    "        context = START + [self._normalize(w) for w in words] + END\n",
    "        for i, word in enumerate(words):\n",
    "            guess = self.tagdict.get(word)\n",
    "            if not guess:\n",
    "                feats = self._get_features(i, word, context, prev, prev2)\n",
    "                guess = self.model.predict(feats)\n",
    "                self.model.update(tags[i], guess, feats)\n",
    "            prev2 = prev; prev = guess\n",
    "\n",
    "    def load(self, loc):\n",
    "        w_td_c = pickle.load(open(loc, 'rb'))\n",
    "        self.model.weights, self.tagdict, self.classes = w_td_c\n",
    "        self.model.classes = self.classes\n",
    "\n",
    "    def _normalize(self, word):\n",
    "        if '-' in word and word[0] != '-':\n",
    "            return '!HYPHEN'\n",
    "        elif word.isdigit() and len(word) == 4:\n",
    "            return '!YEAR'\n",
    "        elif word[0].isdigit():\n",
    "            return '!DIGITS'\n",
    "        else:\n",
    "            return word.lower()\n",
    "\n",
    "    def _get_features(self, i, word, context, prev, prev2):\n",
    "        '''Map tokens into a feature representation, implemented as a\n",
    "        {hashable: float} dict. If the features change, a new model must be\n",
    "        trained.'''\n",
    "        def add(name, *args):\n",
    "            features[' '.join((name,) + tuple(args))] += 1\n",
    "\n",
    "        i += len(START)\n",
    "        features = defaultdict(int)\n",
    "        # It's useful to have a constant feature, which acts sort of like a prior\n",
    "        add('bias')\n",
    "        add('i suffix', word[-3:])\n",
    "        add('i pref1', word[0])\n",
    "        add('i-1 tag', prev)\n",
    "        add('i-2 tag', prev2)\n",
    "        add('i tag+i-2 tag', prev, prev2)\n",
    "        add('i word', context[i])\n",
    "        add('i-1 tag+i word', prev, context[i])\n",
    "        add('i-1 word', context[i-1])\n",
    "        add('i-1 suffix', context[i-1][-3:])\n",
    "        add('i-2 word', context[i-2])\n",
    "        add('i+1 word', context[i+1])\n",
    "        add('i+1 suffix', context[i+1][-3:])\n",
    "        add('i+2 word', context[i+2])\n",
    "        return features\n",
    "\n",
    "    def _make_tagdict(self, sentences):\n",
    "        '''Make a tag dictionary for single-tag words.'''\n",
    "        counts = defaultdict(lambda: defaultdict(int))\n",
    "        for sent in sentences:\n",
    "            words = sent.words()\n",
    "            tags = sent.tags()\n",
    "            for i, word in enumerate(words):\n",
    "                counts[word][tags[i]] += 1\n",
    "                self.classes.add(tag)\n",
    "        freq_thresh = 20\n",
    "        ambiguity_thresh = 0.97\n",
    "        for word, tag_freqs in counts.items():\n",
    "            tag, mode = max(tag_freqs.items(), key=lambda item: item[1])\n",
    "            n = sum(tag_freqs.values())\n",
    "            # Don't add rare words to the tag dictionary\n",
    "            # Only add quite unambiguous words\n",
    "            if n >= freq_thresh and (float(mode) / n) >= ambiguity_thresh:\n",
    "                self.tagdict[word] = tag\n",
    "\n",
    "def _pc(n, d):\n",
    "    return (float(n) / d) * 100\n",
    "\n",
    "\n",
    "def train(parser, sentences, nr_iter):\n",
    "    parser.tagger.start_training(sentences)\n",
    "    for itn in range(nr_iter):\n",
    "        corr = 0; total = 0\n",
    "        random.shuffle(sentences)\n",
    "        for words, gold_tags, gold_parse, gold_label in sentences:\n",
    "            words=sent.words()\n",
    "            gold_tags = sent.pos_tags()\n",
    "            gold_parse = sent.heads()\n",
    "            gold_label = sent.head_labels()\n",
    "            corr += parser.train_one(itn, words, gold_tags, gold_parse)\n",
    "            if itn < 5:\n",
    "                parser.tagger.train_one(words, gold_tags)\n",
    "            total += len(words)\n",
    "        print itn, '%.3f' % (float(corr) / float(total))\n",
    "        if itn == 4:\n",
    "            parser.tagger.model.average_weights()\n",
    "    print 'Averaging weights'\n",
    "    parser.model.average_weights()\n",
    "\n",
    "\n",
    "#use intern function for performance enhancement & pad tokens in the appropriate place\n",
    "def read_conll(loc):\n",
    "    for sent_str in open(loc).read().strip().split('\\n\\n'):\n",
    "        lines = [line.split() for line in sent_str.split('\\n')]\n",
    "        words = DefaultList(''); tags = DefaultList('')\n",
    "        heads = [None]; labels = [None]\n",
    "        for i, (word, pos, head, label) in enumerate(lines):\n",
    "            words.append(intern(word))\n",
    "            #words.append(intern(normalize(word)))\n",
    "            tags.append(intern(pos))\n",
    "            heads.append(int(head) + 1 if head != '-1' else len(lines) + 1)\n",
    "            labels.append(label)\n",
    "        pad_tokens(words); pad_tokens(tags)\n",
    "        yield words, tags, heads, labels\n",
    "\n",
    "\n",
    "def pad_tokens(tokens):\n",
    "    tokens.insert(0, '<start>')\n",
    "    tokens.append('ROOT')\n",
    "\n",
    "\n",
    "def main(model_dir, train_loc, heldout_in, heldout_gold):\n",
    "    if not os.path.exists(model_dir):\n",
    "        os.mkdir(model_dir)\n",
    "\n",
    "    input_sents = list(read_pos(heldout_in))\n",
    "    parser = Parser(load=False)\n",
    "    sentences = list(read_conll(train_loc))\n",
    "    train(parser, sentences, nr_iter=15)\n",
    "    parser.save()\n",
    "    c = 0\n",
    "    t = 0\n",
    "    gold_sents = list(read_conll(heldout_gold))\n",
    "    t1 = time.time()\n",
    "    for (words, tags), (_, _, gold_heads, gold_labels) in zip(input_sents, gold_sents):\n",
    "        _, heads = parser.parse(words)\n",
    "        for i, w in list(enumerate(words))[1:-1]:\n",
    "            if gold_labels[i] in ('P', 'punct'):\n",
    "                continue\n",
    "            if heads[i] == gold_heads[i]:\n",
    "                c += 1\n",
    "            t += 1\n",
    "    t2 = time.time()\n",
    "    print 'Parsing took %0.3f ms' % ((t2-t1)*1000.0)\n",
    "    print c, t, float(c)/t\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main(sys.argv[1], sys.argv[2], sys.argv[3], sys.argv[4])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
