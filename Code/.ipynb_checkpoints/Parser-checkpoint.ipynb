{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "from __future__ imports must occur at the beginning of the file (Full_initial.py, line 139)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"Full_initial.py\"\u001b[0;36m, line \u001b[0;32m139\u001b[0m\n\u001b[0;31m    from __future__ import absolute_import\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m from __future__ imports must occur at the beginning of the file\n"
     ]
    }
   ],
   "source": [
    "from Full_initial import *\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(os.getcwd())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Parse(object):\n",
    "    def __init__(self, n):\n",
    "        self.n = n\n",
    "        self.heads = [None] * (n-1)\n",
    "        self.lefts = []\n",
    "        self.rights = []\n",
    "        for i in range(n+1):\n",
    "            self.lefts.append(DefaultList(0))\n",
    "            self.rights.append(DefaultList(0))\n",
    "    \n",
    "    def add_arc(self, head, child):\n",
    "        self.heads[child] = head\n",
    "        if child < head:\n",
    "            self.lefts[head].append(child)\n",
    "        else:\n",
    "            self.rights[head].append(child)\n",
    "            \n",
    "    SHIFT = 0; RIGHT = 1; LEFT = 2\n",
    "    MOVES = [SHIFT, RIGHT, LEFT]\n",
    " \n",
    "    def transition(move, i, stack, parse):\n",
    "        global SHIFT, RIGHT, LEFT\n",
    "        if move == SHIFT:\n",
    "            stack.append(i)\n",
    "            return i + 1\n",
    "        elif move == RIGHT:\n",
    "            parse.add_arc(stack[-2], stack.pop())\n",
    "            return i\n",
    "        elif move == LEFT:\n",
    "            parse.add_arc(i, stack.pop())\n",
    "            return i\n",
    "        raise GrammarError(\"Unknown move: %d\" % move)\n",
    "        \n",
    "class Spacy_Parser(object):\n",
    "\n",
    "    START = ['-START-', '-START2-']\n",
    "    END = ['-END-', '-END2-']\n",
    "    #AP_MODEL_LOC = os.path.join(os.path.dirname(__file__), PICKLE)\n",
    "\n",
    "    def __init__(self, load=False):\n",
    "        self.tagger = Perc\n",
    "        self.model = AveragedPerceptron()\n",
    "        self.tagdict = {}\n",
    "        self.classes = set()\n",
    "            \n",
    "    def parse(self, words):\n",
    "        tags = self.tagger(words)\n",
    "        n = len(words)\n",
    "        idx = 1\n",
    "        stack = [0]\n",
    "        deps = Parse(n)\n",
    "        while stack or idx < n:\n",
    "            features = extract_features(words, tags, idx, n, stack, deps)\n",
    "            scores = self.model.score(features)\n",
    "            valid_moves = get_valid_moves(i, n, len(stack))\n",
    "            next_move = max(valid_moves, key=lambda move: scores[move])\n",
    "            idx = transition(next_move, idx, stack, parse)\n",
    "        return tags, parse\n",
    " \n",
    "    def train_one(self, itn, words, gold_tags, gold_heads):\n",
    "        n = len(words)\n",
    "        i = 2; stack = [1]; parse = Parse(n)\n",
    "        tags = self.tagger.tag(words)\n",
    "        while stack or (i + 1) < n:\n",
    "            features = extract_features(words, tags, i, n, stack, parse)\n",
    "            scores = self.model.score(features)\n",
    "            valid_moves = get_valid_moves(i, n, len(stack))\n",
    "            guess = max(valid_moves, key=lambda move: scores[move])\n",
    "            gold_moves = get_gold_moves(i, n, stack, parse.heads, gold_heads)\n",
    "            best = max(gold_moves, key=lambda move: scores[move])\n",
    "        self.model.update(best, guess, features)\n",
    "        i = transition(guess, i, stack, parse)\n",
    "        # Return number correct\n",
    "        return len([i for i in range(n-1) if parse.heads[i] == gold_heads[i]])\n",
    "\n",
    "    def get_valid_moves(i, n, stack_depth):\n",
    "        moves = []\n",
    "        if i < n:\n",
    "            moves.append(SHIFT)\n",
    "        if stack_depth <= 2:\n",
    "            moves.append(RIGHT)\n",
    "        if stack_depth <= 1:\n",
    "            moves.append(LEFT)\n",
    "        return moves\n",
    "    \n",
    "def extract_features(words, tags, n0, n, stack, parse):\n",
    "    def get_stack_context(depth, stack, data):\n",
    "        if depth >= 3:\n",
    "            return data[stack[-1]], data[stack[-2]], data[stack[-3]]\n",
    "        elif depth >= 2:\n",
    "            return data[stack[-1]], data[stack[-2]], ''\n",
    "        elif depth == 1:\n",
    "            return data[stack[-1]], '', ''\n",
    "        else:\n",
    "            return '', '', ''\n",
    " \n",
    "    def get_buffer_context(i, n, data):\n",
    "        if i + 1 >= n:\n",
    "            return data[i], '', ''\n",
    "        elif i + 2 >= n:\n",
    "            return data[i], data[i + 1], ''\n",
    "        else:\n",
    "            return data[i], data[i + 1], data[i + 2]\n",
    " \n",
    "    def get_parse_context(word, deps, data):\n",
    "        if word == -1:\n",
    "            return 0, '', ''\n",
    "        deps = deps[word]\n",
    "        valency = len(deps)\n",
    "        if not valency:\n",
    "            return 0, '', ''\n",
    "        elif valency == 1:\n",
    "            return 1, data[deps[-1]], ''\n",
    "        else:\n",
    "            return valency, data[deps[-1]], data[deps[-2]]\n",
    " \n",
    "    features = {}\n",
    "    # Set up the context pieces --- the word, W, and tag, T, of:\n",
    "    # S0-2: Top three words on the stack\n",
    "    # N0-2: First three words of the buffer\n",
    "    # n0b1, n0b2: Two leftmost children of the first word of the buffer\n",
    "    # s0b1, s0b2: Two leftmost children of the top word of the stack\n",
    "    # s0f1, s0f2: Two rightmost children of the top word of the stack\n",
    " \n",
    "    depth = len(stack)\n",
    "    s0 = stack[-1] if depth else -1\n",
    " \n",
    "    Ws0, Ws1, Ws2 = get_stack_context(depth, stack, words)\n",
    "    Ts0, Ts1, Ts2 = get_stack_context(depth, stack, tags)\n",
    " \n",
    "    Wn0, Wn1, Wn2 = get_buffer_context(n0, n, words)\n",
    "    Tn0, Tn1, Tn2 = get_buffer_context(n0, n, tags)\n",
    " \n",
    "    Vn0b, Wn0b1, Wn0b2 = get_parse_context(n0, parse.lefts, words)\n",
    "    Vn0b, Tn0b1, Tn0b2 = get_parse_context(n0, parse.lefts, tags)\n",
    " \n",
    "    Vn0f, Wn0f1, Wn0f2 = get_parse_context(n0, parse.rights, words)\n",
    "    _, Tn0f1, Tn0f2 = get_parse_context(n0, parse.rights, tags)\n",
    " \n",
    "    Vs0b, Ws0b1, Ws0b2 = get_parse_context(s0, parse.lefts, words)\n",
    "    _, Ts0b1, Ts0b2 = get_parse_context(s0, parse.lefts, tags)\n",
    " \n",
    "    Vs0f, Ws0f1, Ws0f2 = get_parse_context(s0, parse.rights, words)\n",
    "    _, Ts0f1, Ts0f2 = get_parse_context(s0, parse.rights, tags)\n",
    " \n",
    "    # Cap numeric features at 5? \n",
    "    # String-distance\n",
    "    Ds0n0 = min((n0 - s0, 5)) if s0 != 0 else 0\n",
    " \n",
    "    features['bias'] = 1\n",
    "    # Add word and tag unigrams\n",
    "    for w in (Wn0, Wn1, Wn2, Ws0, Ws1, Ws2, Wn0b1, Wn0b2, Ws0b1, Ws0b2, Ws0f1, Ws0f2):\n",
    "        if w:\n",
    "            features['w=%s' % w] = 1\n",
    "    for t in (Tn0, Tn1, Tn2, Ts0, Ts1, Ts2, Tn0b1, Tn0b2, Ts0b1, Ts0b2, Ts0f1, Ts0f2):\n",
    "        if t:\n",
    "            features['t=%s' % t] = 1\n",
    " \n",
    "    # Add word/tag pairs\n",
    "    for i, (w, t) in enumerate(((Wn0, Tn0), (Wn1, Tn1), (Wn2, Tn2), (Ws0, Ts0))):\n",
    "        if w or t:\n",
    "            features['%d w=%s, t=%s' % (i, w, t)] = 1\n",
    " \n",
    "    # Add some bigrams\n",
    "    features['s0w=%s,  n0w=%s' % (Ws0, Wn0)] = 1\n",
    "    features['wn0tn0-ws0 %s/%s %s' % (Wn0, Tn0, Ws0)] = 1\n",
    "    features['wn0tn0-ts0 %s/%s %s' % (Wn0, Tn0, Ts0)] = 1\n",
    "    features['ws0ts0-wn0 %s/%s %s' % (Ws0, Ts0, Wn0)] = 1\n",
    "    features['ws0-ts0 tn0 %s/%s %s' % (Ws0, Ts0, Tn0)] = 1\n",
    "    features['wt-wt %s/%s %s/%s' % (Ws0, Ts0, Wn0, Tn0)] = 1\n",
    "    features['tt s0=%s n0=%s' % (Ts0, Tn0)] = 1\n",
    "    features['tt n0=%s n1=%s' % (Tn0, Tn1)] = 1\n",
    " \n",
    "    # Add some tag trigrams\n",
    "    trigrams = ((Tn0, Tn1, Tn2), (Ts0, Tn0, Tn1), (Ts0, Ts1, Tn0), \n",
    "                (Ts0, Ts0f1, Tn0), (Ts0, Ts0f1, Tn0), (Ts0, Tn0, Tn0b1),\n",
    "                (Ts0, Ts0b1, Ts0b2), (Ts0, Ts0f1, Ts0f2), (Tn0, Tn0b1, Tn0b2),\n",
    "                (Ts0, Ts1, Ts1))\n",
    "    for i, (t1, t2, t3) in enumerate(trigrams):\n",
    "        if t1 or t2 or t3:\n",
    "            features['ttt-%d %s %s %s' % (i, t1, t2, t3)] = 1\n",
    " \n",
    "    # Add some valency and distance features\n",
    "    vw = ((Ws0, Vs0f), (Ws0, Vs0b), (Wn0, Vn0b))\n",
    "    vt = ((Ts0, Vs0f), (Ts0, Vs0b), (Tn0, Vn0b))\n",
    "    d = ((Ws0, Ds0n0), (Wn0, Ds0n0), (Ts0, Ds0n0), (Tn0, Ds0n0),\n",
    "        ('t' + Tn0+Ts0, Ds0n0), ('w' + Wn0+Ws0, Ds0n0))\n",
    "    for i, (w_t, v_d) in enumerate(vw + vt + d):\n",
    "        if w_t or v_d:\n",
    "            features['val/d-%d %s %d' % (i, w_t, v_d)] = 1\n",
    "    return features"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
