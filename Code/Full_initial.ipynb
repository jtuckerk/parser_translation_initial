{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spacy Tagger implementation\n",
    "getting aroung 94% accuracy for english and spanish trained on UD data sets ~12,000 training sentence for english, ~7,000? sentences for spanish<br>\n",
    "need to check if I'm doing something wrong, or just need more training samples. Blog claims 97.something% accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from spacy_imp.POS_Tagger import PerceptronTagger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper functions: setup, alignment mapping, test/check...etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convert_corpus_to_sentence_list(corpus):\n",
    "    sentence_list=[]\n",
    "    for sentence in corpus.split(\"\\n\"):\n",
    "        sentence_list.append(sentence.split(\" \"))\n",
    "    return sentence_list\n",
    "\n",
    "def convert_sentence_list_no_tags_to_corpus(sentence_list):\n",
    "    return \"\\n\".join(\" \".join(x) for x in sentence_list)\n",
    "    \n",
    "def convert_tagged_to_train_format(tagged_sent_list):\n",
    "    train_list = []\n",
    "    for sent in tagged_sent_list:\n",
    "        words=[]\n",
    "        tags=[]\n",
    "        for tup in sent:\n",
    "            words.append(tup[0])\n",
    "            tags.append(tup[1])\n",
    "        train_list.append((words,tags))\n",
    "    return train_list\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import codecs\n",
    "#### get training set from UD\n",
    "def load_tagged_sentences(file_name):\n",
    "    sentences_w_tags = []\n",
    "    count = 0\n",
    "    words=[]\n",
    "    tags=[]\n",
    "    on_sentence = False\n",
    "    for line in codecs.open(trainFile, 'r', encoding=\"utf-8\"):\n",
    "    \n",
    "        vals = line.split('\\t')\n",
    "        if (len(vals) > 1):\n",
    "            on_sentence = True\n",
    "            words.append(vals[1])\n",
    "            tags.append(vals[3])\n",
    "        elif (on_sentence):\n",
    "            on_sentence=False\n",
    "            sentences_w_tags.append((words, tags))\n",
    "            words=[]\n",
    "            tags=[]\n",
    "    \n",
    "    return sentences_w_tags # [ ([\"word\", \"word\", \"word\"], [\"tag\", \"tag\", \"tag\"]), next sentece...]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#args sentences_with_tags = [ ([\"word\", \"word\", \"word\"], [\"tag\", \"tag\", \"tag\"]), next sentece...]\n",
    "def train_tagger(tagger, sentences_with_tags, num_iters=5):\n",
    "    print str(len(sentences_with_tags)) + \" training sentences\"\n",
    "    print str(num_iters) + \" training iterations\"\n",
    "    tagger.train(sentences_with_tags, nr_iter=num_iters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import codecs\n",
    "# return arg1 sentences with word/tokens seperated by a \" \" and sentences seperated by \"\\n\" \n",
    "# return arg2 word with tag tuple list\n",
    "def get_test_corpus(file_name):\n",
    "    corpus=\"\"\n",
    "    words=[]\n",
    "    test_correct_tags=[]\n",
    "    sentence_tags = []\n",
    "    sentence_count = 0\n",
    "    on_sentence = False\n",
    "    for line in codecs.open(file_name,'r', encoding=\"utf-8\"):\n",
    "\n",
    "        vals = line.split('\\t')\n",
    "        if (len(vals) > 1):\n",
    "            on_sentence=True\n",
    "            words.append(vals[1])\n",
    "            sentence_tags.append((vals[1],vals[3]))\n",
    "        elif(on_sentence):\n",
    "            sentence_count +=1\n",
    "            on_sentence = False\n",
    "            words.append(\"\\n\")\n",
    "            test_correct_tags.append(sentence_tags)\n",
    "            sentence_tags = []\n",
    "\n",
    "\n",
    "    corpus = \" \".join(words)\n",
    "    print str(sentence_count) + \" sentences in test corpus\"\n",
    "    return corpus, test_correct_tags\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#expects corpus in the same form as get test corpus returns as arg1\n",
    "# returns list [\"word\", \"tag\", float_confidence]\n",
    "def tag_tagger(tagger, corpus, dont_allow=None):\n",
    "    return tagger.tag(corpus, False, dont_allow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import statistics as s\n",
    "import copy\n",
    "\n",
    "#todo get accuracy of tags above certain min_confidence_threshold\n",
    "def analyze_tags(guess_tags, correct_tags, show_full=False, sort_key=lambda ((key_right,key_wrong), value): value):\n",
    "    correct_tag_type ={}\n",
    "    wrong_tag_type = {}\n",
    "    \n",
    "    conf_right = []\n",
    "    conf_wrong = []\n",
    "    \n",
    "    total_tags = 0\n",
    "    total_wrong_tags = 0\n",
    "    \n",
    "    total_sentences = len(guess_tags)\n",
    "    total_wrong_sent = 0\n",
    "    \n",
    "    for sent_num, correct_sentence in enumerate(correct_tags):\n",
    "\n",
    "        perfect_sentence = True\n",
    "        for word_idx, word_tag_tuple in enumerate(correct_sentence):\n",
    "            guess_tuple = guess_tags[sent_num][word_idx]\n",
    "            word = guess_tuple[0]\n",
    "            tag_guess = guess_tuple[1]\n",
    "            guess_confidence = guess_tuple[2]\n",
    "            total_tags +=1\n",
    "            \n",
    "            if(word_tag_tuple[1] != tag_guess):\n",
    "                total_wrong_tags +=1\n",
    "                conf_wrong.append(guess_confidence)\n",
    "                perfect_sentence = False\n",
    "                error_tuple = (word_tag_tuple[1], tag_guess)\n",
    "                wrong_tag_type[error_tuple] = wrong_tag_type.get(error_tuple, 0) + 1\n",
    "            else:\n",
    "                correct_tag_type[tag_guess] = correct_tag_type.get(tag_guess, 0) + 1\n",
    "                conf_right.append(guess_confidence)\n",
    "                \n",
    "        if not perfect_sentence:\n",
    "            total_wrong_sent+= 1\n",
    "    \n",
    "    if(show_full):\n",
    "        for tag_tup, count in sorted(wrong_tag_type.iteritems(),key=sort_key):\n",
    "            print \"correct:\\t\"+tag_tup[0]+\"\\tincorrect:\\t\"+tag_tup[1]+\"\\tcount:\\t\"+str(count)\n",
    "    print total_wrong_sent, total_sentences\n",
    "    \n",
    "    if(len(conf_right) >0 and len(conf_wrong)>0): \n",
    "        print \"average confidence of right = \" + str(s.mean(conf_right))\n",
    "        print \"average confidence of wrong = \" + str(s.mean(conf_wrong))\n",
    "        print \"stdev confidence of right = \" + str(s.stdev(conf_right))\n",
    "        print \"stdev confidence of wrong = \" + str(s.stdev(conf_wrong))\n",
    "   \n",
    "    word_acc = (100.00*(total_tags-total_wrong_tags))/total_tags\n",
    "    sentence_acc = (100.00*(total_sentences-total_wrong_sent))/total_sentences\n",
    "\n",
    "    \n",
    "    print \"token accuracy: \" + str(word_acc) + \"%\"\n",
    "    print \"sentence accuracy: \" + str(sentence_acc) + \"%\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import codecs\n",
    "# loads src and target original documents and loads alignments into list of tuples\n",
    "def get_alignment_info(source_file, tgt_file, align_file, num_matches=1000):\n",
    "    sentence_word_mappings =[]\n",
    "    orig_sentences = []\n",
    "    target_sentences= []\n",
    "    total=0\n",
    "    matches=0\n",
    "\n",
    "    from itertools import izip\n",
    "\n",
    "    with codecs.open(align_file, 'r', encoding=\"utf-8\") as align, codecs.open(source_file, 'r', encoding=\"utf-8\") as orig, codecs.open(tgt_file, 'r', encoding=\"utf-8\") as tgt: \n",
    "        for x, y, z in izip(align, orig, tgt):\n",
    "        \n",
    "            pairings = []\n",
    "            for pair in x.split(\" \"):\n",
    "                indexs = pair.split(\"-\")\n",
    "                if(len(indexs) <=1 or (indexs[0] == \"\" or indexs[1] == \"\")):\n",
    "                    continue\n",
    "                pairings.append((int(indexs[0]), int(indexs[1])))\n",
    "            src_tokens = y.strip().split(\" \")\n",
    "            tgt_tokens = z.strip().split(\" \")\n",
    "            \n",
    "            if (not filter_alignments(src_tokens, tgt_tokens, pairings)):\n",
    "                sentence_word_mappings.append(pairings)\n",
    "                orig_sentences.append(src_tokens)\n",
    "                target_sentences.append(tgt_tokens)\n",
    "                matches+=1\n",
    "         \n",
    "          \n",
    "            if matches>=num_matches:\n",
    "                break\n",
    "            total +=1\n",
    "    print  str((100.0*matches)/total) + \"% left after filter. \"+ str(matches) + \" found after filter\"\n",
    "    print len(orig_sentences)\n",
    "    print len(target_sentences)\n",
    "    print len(sentence_word_mappings)\n",
    "    return orig_sentences, target_sentences, sentence_word_mappings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#some sort of check to see if the alignment is \"good\" enough, filters if not\n",
    "def filter_alignments(src_sent_list, tgt_sent_list, align_pairing_list):\n",
    "    #dont filter any sentences\n",
    "    #return False\n",
    "    \n",
    "    #filter if length of the target and source are different or if the source and pairings lengths dont match\n",
    "    #return not (len(src_sent_list) == len(tgt_sent_list) or len(src_sent_list) == len(align_pairing_list))\n",
    "    \n",
    "    #filter if there are n fewer pairings than words in the target sentence\n",
    "    n=1\n",
    "    return len(tgt_sent_list)-n > len(align_pairing_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "untagged_tag_str = \"NOTAG\"\n",
    "#create a sentence list for training from tagged source language file and maps using alignments to the target language\n",
    "def map_tags(tagged_src, untagged_tgt, alignment_list):\n",
    "    tagged_tgt =[]\n",
    "    for sentence in untagged_tgt:\n",
    "        sent_tag_tuple_list = []\n",
    "        for word in sentence:\n",
    "            sent_tag_tuple_list.append((word, untagged_tag_str))\n",
    "        tagged_tgt.append(sent_tag_tuple_list)\n",
    "            \n",
    "    count = 0\n",
    "    for sent_num, pairings in enumerate(alignment_list):\n",
    "        for pair in pairings:\n",
    "            src_tag_idx = pair[0]\n",
    "            tgt_tag_idx = pair[1]\n",
    "\n",
    "            word = tagged_tgt[sent_num][tgt_tag_idx][0]\n",
    "            tagged_tgt[sent_num][tgt_tag_idx] = (word, tagged_src[sent_num][src_tag_idx][1])\n",
    "    \n",
    "    return tagged_tgt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#remove sentence that have a low overall confidence per word - or maybe sentences that contain 1 or more very\n",
    "# unconfident words - then use that corpus to map to target language \n",
    "def filter_tagged_corpus(tagged_src_sents, untagged_corresp_sents, alignments, avg_threshold, word_conf_cutoff):\n",
    "    to_remove = []\n",
    "    \n",
    "    for sent_num, sentence in enumerate(tagged_src_sents):\n",
    "        conf_sum = 0\n",
    "        removed = False\n",
    "        for word_tag_conf_tup in sentence:\n",
    "            conf = word_tag_conf_tup[2]\n",
    "            conf_sum += conf\n",
    "            if conf < word_conf_cutoff:\n",
    "                removed = True\n",
    "                \n",
    "        if(len(sentence)==0):\n",
    "            removed = True\n",
    "        elif ((1.0*conf_sum)/len(sentence)) < avg_threshold:\n",
    "            removed = True\n",
    "        if removed:\n",
    "            to_remove.append(sent_num)\n",
    "   \n",
    "    orig = len(tagged_src_sents)\n",
    "    left = orig - len(to_remove)\n",
    "    \n",
    "    print(len(tagged_src_sents))\n",
    "    print(len(untagged_corresp_sents))\n",
    "    print(len(alignments))\n",
    "    \n",
    "    print str((100.0*left)/orig) + \"% left after filter. \" + str(left) + \" sentences\"\n",
    "    for idx in reversed(to_remove):\n",
    "        del tagged_src_sents[idx]\n",
    "        del untagged_corresp_sents[idx]\n",
    "        del alignments[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "start1 = \"START1\"\n",
    "start2 = \"START2\"\n",
    "end1 = \"END1\"\n",
    "end2 = \"END2\"\n",
    "def generate_pos_trigrams(tagged_sent_list, ignore_tag=\"\"):\n",
    "    trigram_count_dict = {}\n",
    "    for sentence in tagged_sent_list:\n",
    "        tags = [start1, start2] + [i[1] for i in sentence] + [end1, end2]\n",
    "        for idx in range(len(tags)-2):\n",
    "            tri = tags[idx:idx+3]\n",
    "\n",
    "            if (ignore_tag not in tri):\n",
    "                tri_tup = tuple(tri)\n",
    "                trigram_count_dict[tri_tup] = trigram_count_dict.get(tri_tup, 0) + 1\n",
    "    return trigram_count_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def replace_NOTAG_using_trigram(trigram_dict, partially_tagged_sent_list, notag_str=\"NOTAG\"):\n",
    "    taglist=[]\n",
    "    for key in trigram_dict:\n",
    "        for tag in key:\n",
    "            if not tag in taglist+[start1,start2,end1,end2]:\n",
    "                taglist.append(tag)\n",
    "\n",
    "    for sentence in partially_tagged_sent_list:\n",
    "        tags = [start1, start2] + [i[1] for i in sentence] + [end1, end2]\n",
    "        indeces_of_notag = []\n",
    "\n",
    "        for idx in range(len(tags)-2):\n",
    "            tri = tags[idx:idx+3]\n",
    "\n",
    "            if (notag_str in tri):\n",
    "                \n",
    "                notag_idx = tags.index(notag_str)\n",
    "                if not notag_idx in indeces_of_notag:\n",
    "                    indeces_of_notag.append(notag_idx)\n",
    "        \n",
    "        for notag_index in indeces_of_notag:\n",
    "            #tag, tag, notag\n",
    "            front_tri = tags[notag_index-2:notag_index+1]\n",
    "            mid_tri = tags[notag_index-1:notag_index+2]\n",
    "            back_tri = tags[notag_index:notag_index+3]\n",
    "            \n",
    "            candidate_tag_score_dict = {}\n",
    "            \n",
    "            for tri in [front_tri,mid_tri,back_tri]:\n",
    "                for potential_tag in taglist:\n",
    "                    \n",
    "                    score = trigram_dict.get(tuple([potential_tag if x==notag_str else x for x in tri]),0)\n",
    "                    candidate_tag_score_dict[potential_tag] = candidate_tag_score_dict.get(potential_tag, 0) + score\n",
    "                    \n",
    "            highest_likelyhood_tag = max(candidate_tag_score_dict, key=lambda x: (candidate_tag_score_dict[x],x))\n",
    "\n",
    "            real_idx = notag_index-2\n",
    "            word = sentence[real_idx][0]\n",
    "            sentence[real_idx] = (word, highest_likelyhood_tag)\n",
    "                                         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "untagged_tag_str = \"NOTAG\"\n",
    "\n",
    "#english\n",
    "en_train_file='../Data/UD_English/en-ud-train.conllu'\n",
    "en_test_file='../Data/UD_English/en-ud-test.conllu'\n",
    "\n",
    "#spanish\n",
    "es_train_file='../Data/UD_Spanish/es-ud-train.conllu'\n",
    "es_test_file='../Data/UD_Spanish/es-ud-test.conllu'\n",
    "\n",
    "#arabic...\n",
    "\n",
    "trainFile=en_train_file\n",
    "testFile=en_test_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load, Train and Test source tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "src_language_train_data = load_tagged_sentences(trainFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12543 training sentences\n",
      "5 training interations\n"
     ]
    }
   ],
   "source": [
    "src_language_tagger = PerceptronTagger()\n",
    "train_tagger(src_language_tagger, src_language_train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2077 sentences in test corpus\n"
     ]
    }
   ],
   "source": [
    "src_language_init_test_data, src_test_sentence_w_correct_tags = get_test_corpus(testFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "src_guess_test_tags = tag_tagger(src_language_tagger, src_language_init_test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('What', 'PRON', 5.6899999999999995),\n",
       "  ('if', 'SCONJ', 12.396000000000003),\n",
       "  ('Google', 'PROPN', 12.527),\n",
       "  ('Morphed', 'VERB', 7.921999999999997),\n",
       "  ('Into', 'PROPN', 3.413999999999998),\n",
       "  ('GoogleOS', 'PROPN', 9.038999999999998),\n",
       "  ('?', 'PUNCT', 25.721999999999998)],\n",
       " [('What', 'PRON', 9.245),\n",
       "  ('if', 'SCONJ', 10.305000000000001),\n",
       "  ('Google', 'PROPN', 7.9730000000000025),\n",
       "  ('expanded', 'VERB', 20.623000000000005),\n",
       "  ('on', 'ADP', 21.858999999999995),\n",
       "  ('its', 'PRON', 28.714000000000006),\n",
       "  ('search', 'NOUN', 17.653000000000002),\n",
       "  ('-', 'PUNCT', 19.893999999999995),\n",
       "  ('engine', 'NOUN', 12.19),\n",
       "  ('(', 'PUNCT', 16.424999999999997),\n",
       "  ('and', 'CONJ', 27.615),\n",
       "  ('now', 'ADV', 18.226),\n",
       "  ('e-mail', 'NOUN', 7.347000000000001),\n",
       "  (')', 'PUNCT', 19.79200000000001),\n",
       "  ('wares', 'VERB', 0.9300000000000033),\n",
       "  ('into', 'ADP', 29.33),\n",
       "  ('a', 'DET', 34.970000000000006),\n",
       "  ('full', 'ADJ', 14.674),\n",
       "  ('-', 'PUNCT', 19.303000000000004),\n",
       "  ('fledged', 'ADJ', 1.5500000000000007),\n",
       "  ('operating', 'NOUN', 11.212),\n",
       "  ('system', 'NOUN', 25.064),\n",
       "  ('?', 'PUNCT', 29.928)],\n",
       " [('[', 'PUNCT', 18.492),\n",
       "  ('via', 'ADP', 9.489),\n",
       "  ('Microsoft', 'PROPN', 11.073),\n",
       "  ('Watch', 'PROPN', 7.789999999999997),\n",
       "  ('from', 'ADP', 22.241000000000003),\n",
       "  ('Mary', 'PROPN', 21.010999999999996),\n",
       "  ('Jo', 'PROPN', 25.025999999999996),\n",
       "  ('Foley', 'PROPN', 21.435000000000002),\n",
       "  (']', 'PUNCT', 18.092000000000002)],\n",
       " [('(', 'PUNCT', 20.238999999999997),\n",
       "  ('And', 'CONJ', 15.288000000000007),\n",
       "  (',', 'PUNCT', 27.408999999999995),\n",
       "  ('by', 'ADP', 20.855000000000004),\n",
       "  ('the', 'DET', 35.919000000000004),\n",
       "  ('way', 'NOUN', 27.168000000000003),\n",
       "  (',', 'PUNCT', 31.378),\n",
       "  ('is', 'VERB', 11.842999999999996),\n",
       "  ('anybody', 'NOUN', 8.301999999999996),\n",
       "  ('else', 'ADJ', 13.151999999999992),\n",
       "  ('just', 'ADV', 19.545),\n",
       "  ('a', 'DET', 32.333),\n",
       "  ('little', 'ADJ', 21.423999999999996),\n",
       "  ('nostalgic', 'NOUN', 11.605999999999995),\n",
       "  ('for', 'ADP', 20.690000000000005),\n",
       "  ('the', 'DET', 30.93400000000001),\n",
       "  ('days', 'NOUN', 18.860000000000003),\n",
       "  ('when', 'ADV', 28.974999999999998),\n",
       "  ('that', 'PRON', 13.700000000000001),\n",
       "  ('was', 'VERB', 35.98400000000001),\n",
       "  ('a', 'DET', 27.154999999999994),\n",
       "  ('good', 'ADJ', 25.076999999999998),\n",
       "  ('thing', 'NOUN', 31.790999999999997),\n",
       "  ('?', 'PUNCT', 28.147000000000002),\n",
       "  (')', 'PUNCT', 20.904999999999998)],\n",
       " [('This', 'DET', 2.9460000000000015),\n",
       "  ('BuzzMachine', 'NOUN', 2.586000000000002),\n",
       "  ('post', 'NOUN', 3.4250000000000007),\n",
       "  ('argues', 'NOUN', 2.1290000000000013),\n",
       "  ('that', 'DET', 7.724000000000004),\n",
       "  ('Google', 'PROPN', 16.099999999999998),\n",
       "  (\"'s\", 'PART', 25.462),\n",
       "  ('rush', 'NOUN', 0.629999999999999),\n",
       "  ('toward', 'ADP', 11.975999999999999),\n",
       "  ('ubiquity', 'NOUN', 16.43),\n",
       "  ('might', 'AUX', 14.226000000000003),\n",
       "  ('backfire', 'VERB', 17.123000000000005),\n",
       "  ('--', 'PUNCT', 16.917999999999992),\n",
       "  ('which', 'DET', 28.959000000000003),\n",
       "  ('we', 'PRON', 11.254),\n",
       "  (\"'ve\", 'AUX', 16.495000000000005),\n",
       "  ('all', 'DET', 0.03100000000000591),\n",
       "  ('heard', 'NOUN', 0.6750000000000043),\n",
       "  ('before', 'SCONJ', 2.658000000000005),\n",
       "  (',', 'PUNCT', 29.226999999999997),\n",
       "  ('but', 'CONJ', 22.74499999999999),\n",
       "  ('it', 'PRON', 31.565000000000005),\n",
       "  (\"'s\", 'VERB', 9.066999999999997),\n",
       "  ('particularly', 'ADV', 23.02900000000001),\n",
       "  ('well', 'ADV', 20.498),\n",
       "  ('-', 'PUNCT', 23.201),\n",
       "  ('put', 'VERB', 7.707999999999998),\n",
       "  ('in', 'ADP', 31.186999999999994),\n",
       "  ('this', 'DET', 20.387),\n",
       "  ('post', 'NOUN', 11.775000000000002),\n",
       "  ('.', 'PUNCT', 36.824)]]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_guess_test_tagsess_test_tags[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "916 2078\n",
      "average confidence of right = 20.5697147199\n",
      "average confidence of wrong = 5.8662406232\n",
      "stdev confidence of right = 9.08676816265\n",
      "stdev confidence of wrong = 5.17832108419\n",
      "token accuracy: 93.0945170545%\n",
      "sentence accuracy: 55.9191530318%\n"
     ]
    }
   ],
   "source": [
    "# results\n",
    "analyze_tags(src_guess_test_tags, src_test_sentence_w_correct_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "src_text_file = \"../Data/UN/c.true.en.en_2_es\"\n",
    "tgt_text_file = \"../Data/UN/c.true.es.en_2_es\"\n",
    "align_file = \"../Data/UN/aligned.intersect.en_2_es\"\n",
    "num_sents = 75000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get alignments (do some filtering), tag source language, map to target language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.9008465611% left after filter. 75000 found after filter\n",
      "75000\n",
      "75000\n",
      "75000\n",
      "87.6989099979\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "src_sent_list, tgt_sent_list, alignments_list = get_alignment_info(src_text_file, tgt_text_file, align_file, num_sents)\n",
    "end = time.time()\n",
    "print end-start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tagged_source = tag_tagger(src_language_tagger, convert_sentence_list_no_tags_to_corpus(src_sent_list))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[u'rule', u'22', u'Election'],\n",
       " [u'the',\n",
       "  u'offices',\n",
       "  u'of',\n",
       "  u'the',\n",
       "  u'President',\n",
       "  u'and',\n",
       "  u'Rapporteur',\n",
       "  u'of',\n",
       "  u'the',\n",
       "  u'Conference',\n",
       "  u'shall',\n",
       "  u'normally',\n",
       "  u'be',\n",
       "  u'subject',\n",
       "  u'to',\n",
       "  u'rotation',\n",
       "  u'among',\n",
       "  u'the',\n",
       "  u'five',\n",
       "  u'regional',\n",
       "  u'groups',\n",
       "  u'.'],\n",
       " [u'agenda', u'item', u'124']]"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_sent_list[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75000\n",
      "75000\n",
      "75000\n",
      "26.1053333333% left after filter. 19579 sentences\n"
     ]
    }
   ],
   "source": [
    "filter_tagged_corpus(tagged_source, tgt_sent_list, alignments_list, 20, 0)\n",
    "\n",
    "untagged_target = tgt_sent_list\n",
    "tagged_target_data = map_tags(tagged_source, untagged_target, alignments_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pos_trigram_dict = generate_pos_trigrams(tagged_target_data, \"NOTAG\")\n",
    "\n",
    "replace_NOTAG_using_trigram(pos_trigram_dict, tagged_target_data, \"NOTAG\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(u'los', 'DET'),\n",
       "  (u'cargos', 'NOUN'),\n",
       "  (u'de', 'ADP'),\n",
       "  (u'Presidente', 'PROPN'),\n",
       "  (u'y', 'CONJ'),\n",
       "  (u'Relator', 'PROPN'),\n",
       "  (u'de', 'ADP'),\n",
       "  (u'la', 'DET'),\n",
       "  (u'Conferencia', 'NOUN'),\n",
       "  (u'estar\\xe1n', 'AUX'),\n",
       "  (u'normalmente', 'ADV'),\n",
       "  (u'sujetos', 'ADJ'),\n",
       "  (u'a', 'ADP'),\n",
       "  (u'rotaci\\xf3n', 'NOUN'),\n",
       "  (u'entre', 'ADP'),\n",
       "  (u'los', 'DET'),\n",
       "  (u'cinco', 'ADJ'),\n",
       "  (u'grupos', 'NOUN'),\n",
       "  (u'regionales', 'ADJ'),\n",
       "  (u'.', 'PUNCT')],\n",
       " [(u'en', 'ADP'),\n",
       "  (u'la', 'DET'),\n",
       "  (u'preparaci\\xf3n', 'NOUN'),\n",
       "  (u'de', 'ADP'),\n",
       "  (u'las', 'DET'),\n",
       "  (u'orientaciones', 'NOUN'),\n",
       "  (u'se', 'AUX'),\n",
       "  (u'tuvieron', 'VERB'),\n",
       "  (u'en', 'DET'),\n",
       "  (u'cuenta', 'NOUN'),\n",
       "  (u'las', 'DET'),\n",
       "  (u'consideraciones', 'NOUN'),\n",
       "  (u'siguientes', 'VERB'),\n",
       "  (u':', 'PUNCT')],\n",
       " [(u'junio', 'PROPN'), (u'de', 'PUNCT'), (u'2005', 'NUM')],\n",
       " [(u'primero', 'ADV'),\n",
       "  (u':', 'PUNCT'),\n",
       "  (u'se', 'AUX'),\n",
       "  (u'debe', 'AUX'),\n",
       "  (u'evitar', 'VERB'),\n",
       "  (u'el', 'DET'),\n",
       "  (u'duplicar', 'NOUN'),\n",
       "  (u'esfuerzos', 'NOUN'),\n",
       "  (u'a', 'ADP'),\n",
       "  (u'nivel', 'NOUN'),\n",
       "  (u'internacional', 'ADJ'),\n",
       "  (u'.', 'PUNCT')],\n",
       " [(u'dicha', 'ADJ'),\n",
       "  (u'red', 'NOUN'),\n",
       "  (u'debe', 'AUX'),\n",
       "  (u'basarse', 'VERB'),\n",
       "  (u'en', 'ADP'),\n",
       "  (u'los', 'DET'),\n",
       "  (u'siguientes', 'VERB'),\n",
       "  (u'principios', 'NOUN'),\n",
       "  (u':', 'PUNCT')],\n",
       " [(u'el', 'DET'),\n",
       "  (u'OCDE', 'PROPN'),\n",
       "  (u'es', 'VERB'),\n",
       "  (u'socio', 'DET'),\n",
       "  (u'colaborador', 'NOUN'),\n",
       "  (u'.', 'PUNCT')],\n",
       " [(u'resoluciones', 'NOUN')]]"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagged_target_data[0:7]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train target language tagger on alignment tagged data, Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "target_language_tagger = PerceptronTagger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19579 training sentences\n",
      "5 training iterations\n"
     ]
    }
   ],
   "source": [
    "train_tagger(target_language_tagger, convert_tagged_to_train_format(tagged_target_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14187 sentences in test corpus\n",
      "13696 14188\n",
      "average confidence of right = 16.3619112521\n",
      "average confidence of wrong = 6.81244350911\n",
      "stdev confidence of right = 7.89373733755\n",
      "stdev confidence of wrong = 5.42677599557\n",
      "token accuracy: 81.5577503894%\n",
      "sentence accuracy: 3.46771919932%\n"
     ]
    }
   ],
   "source": [
    "tgt_language_test_data, tgt_test_sentence_w_correct_tags = get_test_corpus(es_train_file)\n",
    "tgt_guess_test_tags = tag_tagger(target_language_tagger, tgt_language_test_data)\n",
    "\n",
    "sort_by_right = lambda ((key_right,key_wrong), value): key_right\n",
    "sort_by_wrong = lambda ((key_right,key_wrong), value): key_wrong\n",
    "sort_by_count = lambda ((key_right,key_wrong), value): value\n",
    "analyze_tags(tgt_guess_test_tags, tgt_test_sentence_w_correct_tags, False, sort_by_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Notes \n",
    "Earlier tests were messed up - getting expected results now.<br><br>\n",
    "\n",
    "15,000 sentence intermediate - filter sentences with a difference in tokens and alignments > n=1 76.07% accuracy <br>\n",
    "15,000 sentence intermediate - filter alignments if != length or source length != alignment_length 63%  <br>\n",
    "15,000 sentence intermediate - no filter 56% <br><br>\n",
    "30,000 sentence intermediate - filter sentences with a difference in tokens and alignments > n=1 76.82% accuracy <br>\n",
    "\n",
    "### filtering target tagged text used in alignment based on confidence: 77.68\n",
    "2 thresholds: avg_sent = average token confidence in a sentence threshold - filter if below <br>\n",
    "min_token = minimum allowed token threshold - filter whole sentence if any token is below<br><br>\n",
    "15000->6513 - n=1 - avg_sent=75% of average correct confidence - min_token=33% of average conf of wrong 74.9% <br> \n",
    "30,000->6141 - n=1 - avg_sent=99% of average correct confidence - min_token=0 of average conf of wrong 75.93% <br>\n",
    "75,000->15,274 - n=1 - avg_sent=99% of average correct confidence - min_token=0 of average conf of wrong 78.16% <br>\n",
    "30,000->13113 - n=1 - avg_sent=75% of average correct confidence - min_token=33% of average conf of wrong 76.57% <br>\n",
    "36,000->14760 - n=1 - avg_sent=0 of average correct confidence - min_token=100% of average conf of wrong 75.88% <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# English Tagger Generated from English Tagged Data\n",
    "### To see how well this should work with perfect alignments\n",
    "#### 83.4% accuracy - down from 93.5% - trained on 15,000 and 12,000 respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15002 training sentences\n",
      "5 training interations\n",
      "2077 sentences in test corpus\n",
      "1573 2078\n",
      "average confidence of right = 18.7751483569\n",
      "average confidence of wrong = 6.81753485577\n",
      "stdev confidence of right = 10.1696628745\n",
      "stdev confidence of wrong = 6.20863312852\n",
      "token accuracy: 83.4236531718%\n",
      "sentence accuracy: 24.302213667%\n"
     ]
    }
   ],
   "source": [
    "en_to_en_tagger = PerceptronTagger()\n",
    "train_tagger(en_to_en_tagger, convert_tagged_to_train_format(tagged_source))\n",
    "en_to_en_test_data, en_to_en_test_sentence_w_correct_tags = get_test_corpus(en_test_file)\n",
    "en_to_en_guess_test_tags = tag_tagger(en_to_en_tagger, en_to_en_test_data)\n",
    "\n",
    "analyze_tags(en_to_en_guess_test_tags, en_to_en_test_sentence_w_correct_tags, False, sort_by_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "acc_id=\"AC5387d3c4597807d2de889091148d126c\"\n",
    "auth_tok=\"1639f28d728c5cd85dfcbd57d231c39c\"\n",
    "\n",
    "from twilio.rest import TwilioRestClient\n",
    " \n",
    "# Find these values at https://twilio.com/user/account\n",
    "account_sid = \"AC5387d3c4597807d2de889091148d126c\"\n",
    "auth_token = \"1639f28d728c5cd85dfcbd57d231c39c\"\n",
    "client = TwilioRestClient(account_sid, auth_token)\n",
    " \n",
    "message = client.messages.create(to=\"+15027949011\", from_=\"+1 502-354-4142\",\n",
    "                                     body=\"done: accuracry = \" + str(accc)+ \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from spacy_imp.POS_Tagger import test1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<spacy_imp.POS_Tagger.PerceptronTagger at 0x109ee2610>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PerceptronTagger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
