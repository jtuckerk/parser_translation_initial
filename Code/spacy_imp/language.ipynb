{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'nlp_jtk' from 'nlp_jtk.pyc'>"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from Parser import *\n",
    "import os\n",
    "from nlp_jtk import Token, Sentence\n",
    "import codecs\n",
    "import Parser\n",
    "reload(Parser)\n",
    "import nlp_jtk\n",
    "reload(nlp_jtk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Language(object):\n",
    "    def __init__(self, name, tagged_file_list=None):\n",
    "        self.tagger_train_iters = 5\n",
    "        self.parser_train_iters = 5 #?\n",
    "        self.name = name\n",
    "        if tagged_file_list is None:\n",
    "            tagged_file_list = []\n",
    "        self.tagged_file_list=tagged_file_list\n",
    "        \n",
    "        self.supervised_parser = Spacy_Parser()\n",
    "        self.unsupervised_parser = Spacy_Parser()\n",
    "        self.train_data_file = None\n",
    "        \n",
    "    def setup(self, train=False, test=False):\n",
    "        if train:\n",
    "            print \"Training supervised \"+self.name+\" Tagger.\"\n",
    "            self.train_supervised_tagger()\n",
    "            print \"Training supervised \"+self.name+\" Parser.\"\n",
    "            #self.train_supervised_parser()\n",
    "                \n",
    "        if test:\n",
    "            print self.name + \" test results: \"\n",
    "            self.test_supervised()\n",
    "        \n",
    "    def train_supervised_tagger(self):\n",
    "        self.train_data = self.get_train_data_set()\n",
    "        print str(len(self.train_data)) + \" tagger training sentences\"\n",
    "        print str(self.tagger_train_iters) + \" training iterations\"\n",
    "        self.supervised_parser.tagger.train(self.train_data, nr_iter=self.tagger_train_iters)\n",
    "        \n",
    "    def train_supervised_parser(self):\n",
    "        if not self.train_data:\n",
    "            print \"need to train tagger first\"\n",
    "            return\n",
    "        \n",
    "        print str(len(self.train_data)) + \" parser training sentences\"\n",
    "        print str(self.parser_train_iters) + \" training iterations\"\n",
    "        self.supervised_parser.train(self.train_data, nr_iter=self.parser_train_iters)\n",
    "        \n",
    "    def test_supervised(self):\n",
    "        test_correct_data = self.get_test_data_set()\n",
    "        test_guess = self.supervised_parser.parse(self.convert_sentence_list_to_untagged_corpus(test_correct_data))\n",
    "        self.get_test_results(test_guess, test_correct_data)\n",
    "        \n",
    "    \n",
    "    def test_unsupervised(self):\n",
    "        test_correct_data = self.get_test_data_set()\n",
    "        test_guess = self.unsupervised_parser.parse(self.convert_sentence_list_to_untagged_corpus(test_correct_data))\n",
    "        self.get_test_results(test_guess, test_correct_data)\n",
    "        pass\n",
    "    \n",
    "    def get_test_results(self, guess_tags, correct_tags):\n",
    "        correct_tag_type ={}\n",
    "        wrong_tag_type = {}\n",
    "    \n",
    "        conf_right = []\n",
    "        conf_wrong = []\n",
    "    \n",
    "        total_tags = 0\n",
    "        total_wrong_tags = 0\n",
    "    \n",
    "        total_sentences = len(guess_tags)\n",
    "        total_wrong_sent = 0\n",
    "    \n",
    "        for sent_num, correct_sentence in enumerate(correct_tags):\n",
    "\n",
    "            perfect_sentence = True\n",
    "            for word_idx, correct_token in enumerate(correct_sentence.get_tokens()):\n",
    "                guess_token = guess_tags[sent_num].get_token_at(word_idx)\n",
    "                word = guess_token.orig\n",
    "                tag_guess = guess_token.pos_tag\n",
    "                guess_confidence = guess_token.conf\n",
    "                total_tags +=1\n",
    "            \n",
    "                if(correct_token.pos_tag != tag_guess):\n",
    "                    total_wrong_tags +=1\n",
    "                    conf_wrong.append(guess_confidence)\n",
    "                    perfect_sentence = False\n",
    "                    error_tuple = (correct_token.pos_tag, tag_guess)\n",
    "                    wrong_tag_type[error_tuple] = wrong_tag_type.get(error_tuple, 0) + 1\n",
    "                else:\n",
    "                    correct_tag_type[tag_guess] = correct_tag_type.get(tag_guess, 0) + 1\n",
    "                    conf_right.append(guess_confidence)\n",
    "                \n",
    "            if not perfect_sentence:\n",
    "                total_wrong_sent+= 1\n",
    "                \n",
    "        if(len(conf_right) >0 and len(conf_wrong)>0): \n",
    "            print \"average confidence of right tag= \" + str(s.mean(conf_right))\n",
    "            print \"average confidence of wrong tag= \" + str(s.mean(conf_wrong))\n",
    "            print \"stdev confidence of right tag= \" + str(s.stdev(conf_right))\n",
    "            print \"stdev confidence of wrong tag= \" + str(s.stdev(conf_wrong))\n",
    "   \n",
    "        tag_word_acc = (100.00*(total_tags-total_wrong_tags))/total_tags\n",
    "        tag_sentence_acc = (100.00*(total_sentences-total_wrong_sent))/total_sentences\n",
    "\n",
    "        print \"tag token accuracy: \" + str(word_acc) + \"%\"\n",
    "        print \"tag sentence accuracy: \" + str(sentence_acc) + \"%\"\n",
    "        print \"have not written tests for parse yet\"\n",
    "    \n",
    "\n",
    "    def get_tagged_sentences(self, file_name):\n",
    "        sentences_w_tags = []\n",
    "        count = 0\n",
    "        words=[]\n",
    "        tags=[]\n",
    "        sentence_obj = Sentence()\n",
    "        on_sentence = False\n",
    "        for line in codecs.open(file_name, 'r', encoding=\"utf-8\"):\n",
    "        \n",
    "            vals = line.split('\\t')\n",
    "            if (len(vals) > 1):\n",
    "                on_sentence = True\n",
    "                tok = Token()\n",
    "                tok.orig = vals[1]\n",
    "                tok.pos_tag = vals[3]\n",
    "                tok.head = int(vals[6])\n",
    "                sentence_obj.add_token(tok)\n",
    "            elif (on_sentence):\n",
    "                on_sentence=False\n",
    "                sentences_w_tags.append(sentence_obj)\n",
    "                sentence_obj = Sentence()\n",
    "    \n",
    "        return sentences_w_tags # [ Sentence_obj, Sentence_obj]\n",
    "    def get_train_data_set(self):\n",
    "        print \"Tagged data: \" + str(len(self.tagged_file_list)) + \"files\"\n",
    "        print \"Picking Largest\"\n",
    "        large_file = \"\"\n",
    "        maxFileSize = 0\n",
    "        for f in self.tagged_file_list:\n",
    "            if os.stat(f).st_size > maxFileSize:\n",
    "                large_file = f\n",
    "                \n",
    "                maxFileSize = os.stat(f).st_size\n",
    "                self.train_data_file = f\n",
    "        print self.train_data_file\n",
    "        return self.get_tagged_sentences(large_file)\n",
    "    \n",
    "    def get_test_data_set(self):\n",
    "        test_list = []\n",
    "        for f in self.tagged_file_list:\n",
    "            if not f == self.train_data_file:\n",
    "                test_list += self.get_tagged_sentences(f)\n",
    "        return test_list\n",
    "    \n",
    "    def convert_sentence_list_to_untagged_corpus(self, sentence_list):\n",
    "        return \"\\n\".join(\" \".join(x.words()) for x in sentence_list)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Translation(object):\n",
    "    def __init__(self, src_language, tgt_language, src_file, tgt_file, align_file):\n",
    "        self.src_language = src_language\n",
    "        self.src_file = src_file\n",
    "        \n",
    "        self.tgt_language = tgt_language\n",
    "        self.tgt_file = tgt_file\n",
    "        \n",
    "        self.align_file = align_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "en_train_file='../../Data/UD_English/en-ud-train.conllu'\n",
    "en_test_file='../../Data/UD_English/en-ud-test.conllu'\n",
    "en_3 = '../../Data/UD_English/en-ud-dev.conllu'\n",
    "en = Language(\"English\", [en_train_file, en_test_file, en_3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English test results: \n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "global name 'i' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-254-f845980f91c0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0men\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-251-5eb36b413faf>\u001b[0m in \u001b[0;36msetup\u001b[0;34m(self, train, test)\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0;32mprint\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\" test results: \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_supervised\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtrain_supervised_tagger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-251-5eb36b413faf>\u001b[0m in \u001b[0;36mtest_supervised\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtest_supervised\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0mtest_correct_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_test_data_set\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m         \u001b[0mtest_guess\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msupervised_parser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_sentence_list_to_untagged_corpus\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_correct_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_test_results\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_guess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_correct_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/tuckerkirven/Desktop/DR-Spring/Code/spacy_imp/Parser.pyc\u001b[0m in \u001b[0;36mparse\u001b[0;34m(self, corpus)\u001b[0m\n\u001b[1;32m     73\u001b[0m                 \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstack\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdeps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m                 \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m                 \u001b[0mvalid_moves\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_valid_moves\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m                 \u001b[0mnext_move\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_moves\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mmove\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmove\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m                 \u001b[0midx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransition\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_move\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstack\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: global name 'i' is not defined"
     ]
    }
   ],
   "source": [
    "en.setup(train=False, test=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
