{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spacy Tagger implementation\n",
    "getting aroung 94% accuracy for english and spanish trained on UD data sets ~12,000 training sentence for english, ~7,000? sentences for spanish<br>\n",
    "need to check if I'm doing something wrong, or just need more training samples. Blog claims 97.something% accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 665,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<module 'spacy_imp.nlp_jtk' from 'spacy_imp/nlp_jtk.py'>"
      ]
     },
     "execution_count": 665,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from spacy_imp.POS_Tagger import PerceptronTagger\n",
    "from spacy_imp.nlp_jtk import Token, Sentence\n",
    "reload(spacy_imp.POS_Tagger)\n",
    "reload(spacy_imp.nlp_jtk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper functions: setup, alignment mapping, test/check...etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 666,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convert_corpus_to_sentence_list(corpus):\n",
    "    sentence_list=[]\n",
    "    for sentence in corpus.split(\"\\n\"):\n",
    "        sentence_list.append(sentence.split(\" \"))\n",
    "    return sentence_list\n",
    "\n",
    "def convert_sentence_list_to_untagged_corpus(sentence_list):\n",
    "    return \"\\n\".join(\" \".join(x.words()) for x in sentence_list)\n",
    "    \n",
    "#obsolete\n",
    "def convert_tagged_to_train_format(tagged_sent_list):\n",
    "    train_list = []\n",
    "    for sent in tagged_sent_list:\n",
    "        words=[]\n",
    "        tags=[]\n",
    "        for tup in sent:\n",
    "            words.append(tup[0])\n",
    "            tags.append(tup[1])\n",
    "        train_list.append((words,tags))\n",
    "    return train_list\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 667,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import codecs\n",
    "#### get training set from UD\n",
    "def load_tagged_sentences(file_name):\n",
    "    sentences_w_tags = []\n",
    "    count = 0\n",
    "    words=[]\n",
    "    tags=[]\n",
    "    sentence_obj = Sentence()\n",
    "    on_sentence = False\n",
    "    for line in codecs.open(file_name, 'r', encoding=\"utf-8\"):\n",
    "    \n",
    "        vals = line.split('\\t')\n",
    "        if (len(vals) > 1):\n",
    "            on_sentence = True\n",
    "            tok = Token()\n",
    "            tok.orig = vals[1]\n",
    "            tok.pos_tag = vals[3]\n",
    "            tok.head = int(vals[6])\n",
    "            sentence_obj.add_token(tok)\n",
    "        elif (on_sentence):\n",
    "            on_sentence=False\n",
    "            sentences_w_tags.append(sentence_obj)\n",
    "            sentence_obj = Sentence()\n",
    "    \n",
    "    return sentences_w_tags # [ Sentence_obj, Sentence_obj]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 668,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#args sentences_with_tags = [ ([\"word\", \"word\", \"word\"], [\"tag\", \"tag\", \"tag\"]), next sentece...]\n",
    "def train_tagger(tagger, sentences_with_tags, num_iters=5):\n",
    "    print str(len(sentences_with_tags)) + \" training sentences\"\n",
    "    print str(num_iters) + \" training iterations\"\n",
    "    tagger.train(sentences_with_tags, nr_iter=num_iters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 669,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import codecs\n",
    "# return arg1 sentences with word/tokens seperated by a \" \" and sentences seperated by \"\\n\" \n",
    "# return arg2 word with tag tuple list\n",
    "def get_test_corpus(file_name):\n",
    "    corpus=\"\"\n",
    "    words=[]\n",
    "    test_correct_tags=[]\n",
    "    sentence_tags = Sentence()\n",
    "    on_sentence = False\n",
    "    for line in codecs.open(file_name,'r', encoding=\"utf-8\"):\n",
    "\n",
    "        vals = line.split('\\t')\n",
    "        if (len(vals) > 1):\n",
    "            on_sentence=True\n",
    "            tok = Token(vals[1],vals[3], vals[6], 0)\n",
    "            sentence_tags.add_token(tok)\n",
    "        elif(on_sentence):\n",
    "            on_sentence = False\n",
    "            test_correct_tags.append(sentence_tags)\n",
    "            sentence_tags = Sentence()\n",
    "\n",
    "\n",
    "    print str(len(test_correct_tags)) + \" sentences in test corpus\"\n",
    "    return test_correct_tags\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 670,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#expects corpus in the same form as get test corpus returns as arg1\n",
    "# returns list of Sentence objects\n",
    "def tag_tagger(tagger, corpus, dont_allow=None):\n",
    "    return tagger.tag(corpus, False, dont_allow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 671,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import statistics as s\n",
    "import copy\n",
    "\n",
    "#todo get accuracy of tags above certain min_confidence_threshold\n",
    "def analyze_tags(guess_tags, correct_tags, show_full=False, sort_key=lambda ((key_right,key_wrong), value): value):\n",
    "    correct_tag_type ={}\n",
    "    wrong_tag_type = {}\n",
    "    \n",
    "    conf_right = []\n",
    "    conf_wrong = []\n",
    "    \n",
    "    total_tags = 0\n",
    "    total_wrong_tags = 0\n",
    "    \n",
    "    total_sentences = len(guess_tags)\n",
    "    total_wrong_sent = 0\n",
    "    \n",
    "    for sent_num, correct_sentence in enumerate(correct_tags):\n",
    "\n",
    "        perfect_sentence = True\n",
    "        for word_idx, correct_token in enumerate(correct_sentence.get_tokens()):\n",
    "            guess_token = guess_tags[sent_num].get_token_at(word_idx)\n",
    "            word = guess_token.orig\n",
    "            tag_guess = guess_token.pos_tag\n",
    "            guess_confidence = guess_token.conf\n",
    "            total_tags +=1\n",
    "            \n",
    "            if(correct_token.pos_tag != tag_guess):\n",
    "                total_wrong_tags +=1\n",
    "                conf_wrong.append(guess_confidence)\n",
    "                perfect_sentence = False\n",
    "                error_tuple = (correct_token.pos_tag, tag_guess)\n",
    "                wrong_tag_type[error_tuple] = wrong_tag_type.get(error_tuple, 0) + 1\n",
    "            else:\n",
    "                correct_tag_type[tag_guess] = correct_tag_type.get(tag_guess, 0) + 1\n",
    "                conf_right.append(guess_confidence)\n",
    "                \n",
    "        if not perfect_sentence:\n",
    "            total_wrong_sent+= 1\n",
    "    \n",
    "    if(show_full):\n",
    "        for tag_tup, count in sorted(wrong_tag_type.iteritems(),key=sort_key):\n",
    "            print \"correct:\\t\"+tag_tup[0]+\"\\tincorrect:\\t\"+tag_tup[1]+\"\\tcount:\\t\"+str(count)\n",
    "    print total_wrong_sent, total_sentences\n",
    "    \n",
    "    if(len(conf_right) >0 and len(conf_wrong)>0): \n",
    "        print \"average confidence of right = \" + str(s.mean(conf_right))\n",
    "        print \"average confidence of wrong = \" + str(s.mean(conf_wrong))\n",
    "        print \"stdev confidence of right = \" + str(s.stdev(conf_right))\n",
    "        print \"stdev confidence of wrong = \" + str(s.stdev(conf_wrong))\n",
    "   \n",
    "    word_acc = (100.00*(total_tags-total_wrong_tags))/total_tags\n",
    "    sentence_acc = (100.00*(total_sentences-total_wrong_sent))/total_sentences\n",
    "\n",
    "    \n",
    "    print \"token accuracy: \" + str(word_acc) + \"%\"\n",
    "    print \"sentence accuracy: \" + str(sentence_acc) + \"%\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 672,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import codecs\n",
    "# loads src and target original documents and loads alignments into list of tuples\n",
    "def get_alignment_info(source_file, tgt_file, align_file, num_matches=1000):\n",
    "    sentence_word_mappings =[]\n",
    "    orig_sentences = []\n",
    "    target_sentences= []\n",
    "    total=0\n",
    "    matches=0\n",
    "\n",
    "    from itertools import izip\n",
    "\n",
    "    with codecs.open(align_file, 'r', encoding=\"utf-8\") as align, codecs.open(source_file, 'r', encoding=\"utf-8\") as orig, codecs.open(tgt_file, 'r', encoding=\"utf-8\") as tgt: \n",
    "        for x, y, z in izip(align, orig, tgt):\n",
    "        \n",
    "            pairings = []\n",
    "            for pair in x.split(\" \"):\n",
    "                indexs = pair.split(\"-\")\n",
    "                if(len(indexs) <=1 or (indexs[0] == \"\" or indexs[1] == \"\")):\n",
    "                    continue\n",
    "                pairings.append((int(indexs[0]), int(indexs[1])))\n",
    "            src_tokens = y.strip().split(\" \")\n",
    "            tgt_tokens = z.strip().split(\" \")\n",
    "            \n",
    "            if (not filter_alignments(src_tokens, tgt_tokens, pairings)):\n",
    "                sentence_word_mappings.append(pairings)\n",
    "                orig_sentences.append(src_tokens)\n",
    "                target_sentences.append(tgt_tokens)\n",
    "                matches+=1\n",
    "         \n",
    "          \n",
    "            if matches>=num_matches:\n",
    "                break\n",
    "            total +=1\n",
    "    print  str((100.0*matches)/total) + \"% left after filter. \"+ str(matches) + \" found after filter\"\n",
    "    print len(orig_sentences)\n",
    "    print len(target_sentences)\n",
    "    print len(sentence_word_mappings)\n",
    "    return orig_sentences, target_sentences, sentence_word_mappings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 673,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#some sort of check to see if the alignment is \"good\" enough, filters if not\n",
    "def filter_alignments(src_sent_list, tgt_sent_list, align_pairing_list):\n",
    "    #dont filter any sentences\n",
    "    #return False\n",
    "    \n",
    "    #filter if length of the target and source are different or if the source and pairings lengths dont match\n",
    "    #return not (len(src_sent_list) == len(tgt_sent_list) or len(src_sent_list) == len(align_pairing_list))\n",
    "    \n",
    "    #filter if there are n fewer pairings than words in the target sentence\n",
    "    n=1\n",
    "    return len(tgt_sent_list)-n > len(align_pairing_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 674,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "untagged_tag_str = \"NOTAG\"\n",
    "#create a sentence list for training from tagged source language file and maps using alignments to the target language\n",
    "def map_tags(tagged_src, untagged_tgt, alignment_list):\n",
    "    tagged_tgt =[]\n",
    "    for sentence in untagged_tgt:\n",
    "        sent_token_list = Sentence()\n",
    "        for word in sentence:\n",
    "            sent_token_list.add_token(Token(word, untagged_tag_str, 0, 0))\n",
    "            \n",
    "        tagged_tgt.append(sent_token_list)\n",
    "            \n",
    "    count = 0\n",
    "    for sent_num, pairings in enumerate(alignment_list):\n",
    "        for pair in pairings:\n",
    "            src_tag_idx = pair[0]\n",
    "            tgt_tag_idx = pair[1]\n",
    "\n",
    "            tagged_tgt[sent_num].get_token_at(tgt_tag_idx).pos_tag = tagged_src[sent_num].get_token_at(src_tag_idx).pos_tag\n",
    "    \n",
    "    return tagged_tgt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 675,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#remove sentence that have a low overall confidence per word - or maybe sentences that contain 1 or more very\n",
    "# unconfident words - then use that corpus to map to target language \n",
    "def filter_tagged_corpus(tagged_src_sents, untagged_corresp_sents, alignments, avg_threshold, word_conf_cutoff):\n",
    "    to_remove = []\n",
    "    \n",
    "    for sent_num, sentence in enumerate(tagged_src_sents):\n",
    "        conf_sum = 0\n",
    "        removed = False\n",
    "        for tok in sentence.get_tokens():\n",
    "            conf_sum +=  tok.conf\n",
    "            if tok.conf < word_conf_cutoff:\n",
    "                removed = True\n",
    "                \n",
    "        if(len(sentence)==0):\n",
    "            removed = True\n",
    "        elif ((1.0*conf_sum)/len(sentence)) < avg_threshold:\n",
    "            removed = True\n",
    "        if removed:\n",
    "            to_remove.append(sent_num)\n",
    "   \n",
    "    orig = len(tagged_src_sents)\n",
    "    left = orig - len(to_remove)\n",
    "    \n",
    "    print(len(tagged_src_sents))\n",
    "    print(len(untagged_corresp_sents))\n",
    "    print(len(alignments))\n",
    "    \n",
    "    print str((100.0*left)/orig) + \"% left after filter. \" + str(left) + \" sentences\"\n",
    "    for idx in reversed(to_remove):\n",
    "        del tagged_src_sents[idx]\n",
    "        del untagged_corresp_sents[idx]\n",
    "        del alignments[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 676,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "start1 = \"START1\"\n",
    "start2 = \"START2\"\n",
    "end1 = \"END1\"\n",
    "end2 = \"END2\"\n",
    "def generate_pos_trigrams(tagged_sent_list, ignore_tag=\"\"):\n",
    "    trigram_count_dict = {}\n",
    "    for sentence in tagged_sent_list:\n",
    "        tags = [start1, start2] + [tok.pos_tag for tok in sentence.get_tokens()] + [end1, end2]\n",
    "        for idx in range(len(tags)-2):\n",
    "            tri = tags[idx:idx+3]\n",
    "\n",
    "            if (ignore_tag not in tri):\n",
    "                tri_tup = tuple(tri)\n",
    "                trigram_count_dict[tri_tup] = trigram_count_dict.get(tri_tup, 0) + 1\n",
    "    return trigram_count_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 677,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def replace_NOTAG_using_trigram(trigram_dict, partially_tagged_sent_list, notag_str=\"NOTAG\"):\n",
    "    taglist=[]\n",
    "    for key in trigram_dict:\n",
    "        for tag in key:\n",
    "            if not tag in taglist+[start1,start2,end1,end2]:\n",
    "                taglist.append(tag)\n",
    "\n",
    "    for sentence in partially_tagged_sent_list:\n",
    "        tags = [start1, start2] + [tok.pos_tag for tok in sentence.get_tokens()] + [end1, end2]\n",
    "        indeces_of_notag = []\n",
    "\n",
    "        for idx in range(len(tags)-2):\n",
    "            tri = tags[idx:idx+3]\n",
    "\n",
    "            if (notag_str in tri):\n",
    "                \n",
    "                notag_idx = tags.index(notag_str)\n",
    "                if not notag_idx in indeces_of_notag:\n",
    "                    indeces_of_notag.append(notag_idx)\n",
    "        \n",
    "        for notag_index in indeces_of_notag:\n",
    "            #tag, tag, notag\n",
    "            front_tri = tags[notag_index-2:notag_index+1]\n",
    "            mid_tri = tags[notag_index-1:notag_index+2]\n",
    "            back_tri = tags[notag_index:notag_index+3]\n",
    "            \n",
    "            candidate_tag_score_dict = {}\n",
    "            \n",
    "            for tri in [front_tri,mid_tri,back_tri]:\n",
    "                for potential_tag in taglist:\n",
    "                    \n",
    "                    score = trigram_dict.get(tuple([potential_tag if x==notag_str else x for x in tri]),0)\n",
    "                    candidate_tag_score_dict[potential_tag] = candidate_tag_score_dict.get(potential_tag, 0) + score\n",
    "                    \n",
    "            highest_likelyhood_tag = max(candidate_tag_score_dict, key=lambda x: (candidate_tag_score_dict[x],x))\n",
    "\n",
    "            real_idx = notag_index-2\n",
    "\n",
    "            sentence.get_token_at(real_idx).pos_tag = highest_likelyhood_tag\n",
    "                                         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 640,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "untagged_tag_str = \"NOTAG\"\n",
    "\n",
    "#english\n",
    "en_train_file='../Data/UD_English/en-ud-train.conllu'\n",
    "en_test_file='../Data/UD_English/en-ud-test.conllu'\n",
    "\n",
    "#spanish\n",
    "es_train_file='../Data/UD_Spanish/es-ud-train.conllu'\n",
    "es_test_file='../Data/UD_Spanish/es-ud-test.conllu'\n",
    "\n",
    "#arabic...\n",
    "\n",
    "trainFile=en_train_file\n",
    "testFile=en_test_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load, Train and Test source tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 596,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "src_language_train_data = load_tagged_sentences(trainFile)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 597,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12543"
      ]
     },
     "execution_count": 597,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(src_language_train_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 598,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12543 training sentences\n",
      "5 training iterations\n",
      "[<spacy_imp.nlp_jtk.Sentence object at 0x14815a8d0>, <spacy_imp.nlp_jtk.Sentence object at 0x14815a650>, <spacy_imp.nlp_jtk.Sentence object at 0x159415c50>]\n",
      "<spacy_imp.nlp_jtk.Sentence object at 0x14815a210>\n",
      "214.073837042\n"
     ]
    }
   ],
   "source": [
    "src_language_tagger = PerceptronTagger()\n",
    "import time \n",
    "start = time.time()\n",
    "train_tagger(src_language_tagger, src_language_train_data, 5)\n",
    "end = time.time()\n",
    "print end-start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 599,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2077 sentences in test corpus\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<spacy_imp.nlp_jtk.Sentence at 0x159b4b1d0>,\n",
       " <spacy_imp.nlp_jtk.Sentence at 0x14838a490>,\n",
       " <spacy_imp.nlp_jtk.Sentence at 0x14838af10>]"
      ]
     },
     "execution_count": 599,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_test_sentence_w_correct_tags = get_test_corpus(testFile)\n",
    "src_test_sentence_w_correct_tags[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 600,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "src_guess_test_tags = tag_tagger(src_language_tagger, convert_sentence_list_to_untagged_corpus(src_test_sentence_w_correct_tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 601,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "908 2077\n",
      "average confidence of right = 20.3380896959\n",
      "average confidence of wrong = 5.90585264997\n",
      "stdev confidence of right = 8.92102520046\n",
      "stdev confidence of wrong = 5.13833976104\n",
      "token accuracy: 93.1582722346%\n",
      "sentence accuracy: 56.2831006259%\n"
     ]
    }
   ],
   "source": [
    "# results\n",
    "analyze_tags(src_guess_test_tags, src_test_sentence_w_correct_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 689,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "src_text_file = \"../Data/UN/c.true.en.en_2_es\"\n",
    "tgt_text_file = \"../Data/UN/c.true.es.en_2_es\"\n",
    "align_file = \"../Data/UN/aligned.intersect.en_2_es\"\n",
    "num_sents = 60000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get alignments (do some filtering), tag source language, map to target language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 690,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.9755042269% left after filter. 60000 found after filter\n",
      "60000\n",
      "60000\n",
      "60000\n",
      "80.2812511921\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "src_sent_list, tgt_sent_list, alignments_list = get_alignment_info(src_text_file, tgt_text_file, align_file, num_sents)\n",
    "end = time.time()\n",
    "print end-start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 691,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tagged_source = tag_tagger(src_language_tagger, convert_sentence_list_no_tags_to_corpus(src_sent_list))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 692,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[u'rule', u'22', u'Election'],\n",
       " [u'the',\n",
       "  u'offices',\n",
       "  u'of',\n",
       "  u'the',\n",
       "  u'President',\n",
       "  u'and',\n",
       "  u'Rapporteur',\n",
       "  u'of',\n",
       "  u'the',\n",
       "  u'Conference',\n",
       "  u'shall',\n",
       "  u'normally',\n",
       "  u'be',\n",
       "  u'subject',\n",
       "  u'to',\n",
       "  u'rotation',\n",
       "  u'among',\n",
       "  u'the',\n",
       "  u'five',\n",
       "  u'regional',\n",
       "  u'groups',\n",
       "  u'.'],\n",
       " [u'agenda', u'item', u'124']]"
      ]
     },
     "execution_count": 692,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_sent_list[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 693,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000\n",
      "60000\n",
      "60000\n",
      "23.4133333333% left after filter. 14048 sentences\n"
     ]
    }
   ],
   "source": [
    "filter_tagged_corpus(tagged_source, tgt_sent_list, alignments_list, 20, 0)\n",
    "\n",
    "untagged_target = tgt_sent_list\n",
    "tagged_target_data = map_tags(tagged_source, untagged_target, alignments_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 694,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pos_trigram_dict = generate_pos_trigrams(tagged_target_data, \"NOTAG\")\n",
    "\n",
    "replace_NOTAG_using_trigram(pos_trigram_dict, tagged_target_data, \"NOTAG\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 695,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<spacy_imp.nlp_jtk.Sentence at 0x14495be50>,\n",
       " <spacy_imp.nlp_jtk.Sentence at 0x14495bb10>,\n",
       " <spacy_imp.nlp_jtk.Sentence at 0x14495b610>,\n",
       " <spacy_imp.nlp_jtk.Sentence at 0x14495b050>,\n",
       " <spacy_imp.nlp_jtk.Sentence at 0x14495b0d0>,\n",
       " <spacy_imp.nlp_jtk.Sentence at 0x14495b150>,\n",
       " <spacy_imp.nlp_jtk.Sentence at 0x14495b110>]"
      ]
     },
     "execution_count": 695,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagged_target_data[0:7]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train target language tagger on alignment tagged data, Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 696,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "target_language_tagger = PerceptronTagger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 697,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14048 training sentences\n",
      "5 training iterations\n",
      "[<spacy_imp.nlp_jtk.Sentence object at 0x14495bb10>, <spacy_imp.nlp_jtk.Sentence object at 0x14495b610>, <spacy_imp.nlp_jtk.Sentence object at 0x14495b050>]\n",
      "<spacy_imp.nlp_jtk.Sentence object at 0x14495be50>\n"
     ]
    }
   ],
   "source": [
    "train_tagger(target_language_tagger, tagged_target_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 698,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14187 sentences in test corpus\n",
      "13721 14187\n",
      "average confidence of right = 15.9085607475\n",
      "average confidence of wrong = 6.57641687362\n",
      "stdev confidence of right = 7.74871842437\n",
      "stdev confidence of wrong = 5.34872684873\n",
      "token accuracy: 81.5046330154%\n",
      "sentence accuracy: 3.28469725805%\n"
     ]
    }
   ],
   "source": [
    "tgt_test_sentence_w_correct_tags = get_test_corpus(es_train_file)\n",
    "tgt_guess_test_tags = tag_tagger(target_language_tagger, convert_sentence_list_to_untagged_corpus(tgt_test_sentence_w_correct_tags))\n",
    "\n",
    "sort_by_right = lambda ((key_right,key_wrong), value): key_right\n",
    "sort_by_wrong = lambda ((key_right,key_wrong), value): key_wrong\n",
    "sort_by_count = lambda ((key_right,key_wrong), value): value\n",
    "analyze_tags(tgt_guess_test_tags, tgt_test_sentence_w_correct_tags, False, sort_by_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Notes \n",
    "Earlier tests were messed up - getting expected results now.<br><br>\n",
    "\n",
    "15,000 sentence intermediate - filter sentences with a difference in tokens and alignments > n=1 76.07% accuracy <br>\n",
    "15,000 sentence intermediate - filter alignments if != length or source length != alignment_length 63%  <br>\n",
    "15,000 sentence intermediate - no filter 56% <br><br>\n",
    "30,000 sentence intermediate - filter sentences with a difference in tokens and alignments > n=1 76.82% accuracy <br>\n",
    "\n",
    "### filtering target tagged text used in alignment based on confidence: 77.68\n",
    "2 thresholds: avg_sent = average token confidence in a sentence threshold - filter if below <br>\n",
    "min_token = minimum allowed token threshold - filter whole sentence if any token is below<br><br>\n",
    "15000->6513 - n=1 - avg_sent=75% of average correct confidence - min_token=33% of average conf of wrong 74.9% <br> \n",
    "30,000->6141 - n=1 - avg_sent=99% of average correct confidence - min_token=0 of average conf of wrong 75.93% <br>\n",
    "75,000->15,274 - n=1 - avg_sent=99% of average correct confidence - min_token=0 of average conf of wrong 78.16% <br>\n",
    "30,000->13113 - n=1 - avg_sent=75% of average correct confidence - min_token=33% of average conf of wrong 76.57% <br>\n",
    "36,000->14760 - n=1 - avg_sent=0 of average correct confidence - min_token=100% of average conf of wrong 75.88% <br>\n",
    "\n",
    "### using trigram to tag untagged tokens before training: \n",
    "75,000->14,187 - n=1 - avg_sent=99% of average correct confidence - min_token=0 of average conf of wrong 81.55% <br>\n",
    "\n",
    "### Alignment issues (src,tgt)\n",
    "['adoption', 'of', 'the', 'agenda', 'and', 'other', 'organizational', 'matters']<br>\n",
    "['aprobación', 'del', 'programa', 'y', 'otras', 'cuestiones', 'de', 'organización']<br>\n",
    "[(0, 0), (1, 1), (3, 2), (4, 3), (5, 4), (6, 7), (7, 5)]<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# English Tagger Generated from English Tagged Data\n",
    "### To see how well this should work with perfect alignments\n",
    "#### 83.4% accuracy - down from 93.5% - trained on 15,000 and 12,000 respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15002 training sentences\n",
      "5 training interations\n",
      "2077 sentences in test corpus\n",
      "1573 2078\n",
      "average confidence of right = 18.7751483569\n",
      "average confidence of wrong = 6.81753485577\n",
      "stdev confidence of right = 10.1696628745\n",
      "stdev confidence of wrong = 6.20863312852\n",
      "token accuracy: 83.4236531718%\n",
      "sentence accuracy: 24.302213667%\n"
     ]
    }
   ],
   "source": [
    "en_to_en_tagger = PerceptronTagger()\n",
    "train_tagger(en_to_en_tagger, convert_tagged_to_train_format(tagged_source))\n",
    "en_to_en_test_data, en_to_en_test_sentence_w_correct_tags = get_test_corpus(en_test_file)\n",
    "en_to_en_guess_test_tags = tag_tagger(en_to_en_tagger, en_to_en_test_data)\n",
    "\n",
    "analyze_tags(en_to_en_guess_test_tags, en_to_en_test_sentence_w_correct_tags, False, sort_by_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "acc_id=\"AC5387d3c4597807d2de889091148d126c\"\n",
    "auth_tok=\"1639f28d728c5cd85dfcbd57d231c39c\"\n",
    "\n",
    "from twilio.rest import TwilioRestClient\n",
    " \n",
    "# Find these values at https://twilio.com/user/account\n",
    "account_sid = \"AC5387d3c4597807d2de889091148d126c\"\n",
    "auth_token = \"1639f28d728c5cd85dfcbd57d231c39c\"\n",
    "client = TwilioRestClient(account_sid, auth_token)\n",
    " \n",
    "message = client.messages.create(to=\"+15027949011\", from_=\"+1 502-354-4142\",\n",
    "                                     body=\"done: accuracry = \" + str(accc)+ \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from spacy_imp.POS_Tagger import test1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<spacy_imp.POS_Tagger.PerceptronTagger at 0x109ee2610>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PerceptronTagger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
